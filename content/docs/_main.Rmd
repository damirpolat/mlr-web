---
title: "Visualization of predictions"
authors: ["jakob-richter"]
date: 2015-07-28
categories: ["R", "r-bloggers"]
tags: ["visualization", "prediction", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

In this post I want to shortly introduce you to the great visualization possibilities of `mlr`.
Within the last months a lot of work has been put into that field.
This post is not a [tutorial](https://mlr.mlr-org.com/) but more a demonstration of how little code you have to write with `mlr` to get some nice plots showing the prediction behaviors for different learners.

<!--more-->

First we define a list containing all the [learners](https://mlr.mlr-org.com/articles/tutorial/devel/integrated_learners.html) we want to visualize.
Notice that most of the `mlr` methods are able to work with just the string (i.e. `"classif.svm"`) to know what learner you mean.
Nevertheless you can define the learner more precisely with `makeLearner()` and set some parameters such as the `kernel` in this example.

First we define the list of learners we want to visualize.
```{r list-all-learners, message = FALSE}
library(mlr)
learners = list(
  makeLearner("classif.svm", kernel = "linear"),
  makeLearner("classif.svm", kernel = "polynomial"),
  makeLearner("classif.svm", kernel = "radial"),
  "classif.qda",
  "classif.randomForest",
  "classif.knn"
  )
```

## Support Vector Machines
Now lets have a look at the different results and lets start with the SVM with a *linear kernel*.

```{r, linear-svm}
plotLearnerPrediction(learner = learners[[1]], task = iris.task)
```

We can see clearly that in fact the decision boundary is indeed linear.
Furthermore the misclassified items are highlighted and a 10-fold cross validation to obtain the mean missclassification error is executed.

For the *polynomial* and the *radial kernel* the decision boundaries already look a bit more sophisticated:
```{r, polynomial-radial-svm}
plotLearnerPrediction(learner = learners[[2]], task = iris.task)
plotLearnerPrediction(learner = learners[[3]], task = iris.task)
```

Note that the intensity of the colors also indicates the certainty of the prediction and that this example is probably a rare case where the linear kernel performs best. although this is likely only the case because we didn't optimize the parameters for the radial kernel.

## Quadratic Discriminant Analysis
```{r, qda}
plotLearnerPrediction(learner = learners[[4]], task = iris.task)
```

A well known classificator from the basic course of statistics delivers a similar performance as the SVMs.

## Random Forest
```{r, randomforest}
plotLearnerPrediction(learner = learners[[5]], task = iris.task)
```

A completely different picture is generated by the random forest.
Here you can see that the whole data set is used to generate the model and as a result it looks like it gives a perfect fit but obviously you wouldn't use the train data to evaluate your model.
And the results of the 10-fold cross validation indicate that the random forest is actually not better then the others.

## Nearest Neighbour
```{r, knn}
plotLearnerPrediction(learner = learners[[6]], task = iris.task)
```

In the default setting knn just look for 'k=1' neighbor and as a result the classifier does not return probabilities but only the class labels.


<!--chapter:end:2015-07-28-Visualisation-of-predictions.Rmd-->

---
title: "Benchmarking mlr (default) learners on OpenML"
authors: ["philipp-probst"]
date: 2016-08-11
categories: ["R", "r-bloggers"]
tags: ["OpenML", "benchmark", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

There are already some benchmarking studies about different classification algorithms out there. The probably most well known and 
most extensive one is the 
[Do we Need Hundreds of Classifers to Solve Real World Classication Problems?](http://www.jmlr.org/papers/volume15/delgado14a/source/delgado14a.pdf)
paper. They use different software and also different tuning processes to compare 179 learners on more than 121 datasets, mainly 
from the [UCI](https://archive.ics.uci.edu/ml/datasets.html) site. They exclude different datasets, because their dimension 
(number of observations or number of features) are too high, they are not in a proper format or because of other reasons. 
There are also summarized some criticism about the representability of the datasets and the generability of benchmarking results. 
It remains a bit unclear if their tuning process is done also on the test data or only on the training data (page 3154). 
They reported the random forest algorithms to be the best one (in general) for multiclass classification datasets and 
the support vector machine (svm) the second best one. On binary class classification tasks neural networks also perform 
competitively. They recommend the R library **caret** for choosing a classifier. 

Other benchmarking studies use much less datasets and are much less extensive (e.g. the 
[Caruana Paper](https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf)). Computational power was also not the same 
on these days. 

In my first approach for benchmarking different learners I follow a more standardized approach, that can be easily 
redone in future when new learners or datasets are added to the analysis. 
I use the R package **OpenML** for getting access to OpenML datasets and the R package **mlr** (similar to caret, but more extensive) to have a standardized interface to machine learning algorithms in R. 
Furthermore the experiments are done with the help of the package [**batchtools**](https://github.com/mllg/batchtools), 
in order to parallelize the experiments (Installation via **devtools** package: devtools::install_github("mllg/batchtools")).

The first step is to choose some datasets of the OpenML dataplatform. This is done in the [datasets.R](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/code/datasets.R)
file. I want to evaluate classification learners as well as regression learners, so I choose datasets for both tasks. 
The choosing date was 28.01.2016 so probably nowadays there are more available. I applied several exclusion criteria:

1. No datasets with missing values (this can be omitted in future with some imputation technique)
2. Each dataset only once (although there exist several tasks for some)
3. Exclusion of datasets that were obviously artificially created (e.g. some artificial datasets created by Friedman)
4. Only datasets with number of observations and number of features smaller than 1000 (this is done to get a first fast analysis; 
bigger datasets are added later)
5. In the classification case: only datasets where the target is a factor

Of course this exclusion criteria change the representativeness of the datasets.  

These exclusion criteria provide 184 classification datasets and 98 regression datasets. 

The benchmark file on these datasets can be found
[here](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/code/benchmark.R).
For the classification datasets all available classification learners in mlr, that 
can handle multiclass problems, provide probability estimations and can handle factor features, are used. "boosting" of the 
**adabag** package is excluded, because it took too long on our test dataset. 

For the regression datasets only regression learners that can handle factor features are included. 
The learners "btgp", "btgpllm" and "btlm" are excluded, because their training time was too long. 

In this preliminary study all learners are used with their default hyperparameter settings without tuning. 
The evaluation technique is 10-fold crossvalidation, 10 times repeated and it is executed by the resample function 
in mlr. The folds are the same for all the learners. The evaluation measures are the accuracy, the balanced error rate,
the (multiclass) auc, the (multiclass) brier score and the logarithmic loss for the classification and
the mean square error, mean of absolute error, median of square error and median of absolute error. Additionally the 
training time is recorded. 

On 12 cores it took me around 4 days for all datasets. 

I evaluate the results with help of the **data.table** package, which is good for handling big datasets and fast calculation 
of subset statistics. Graphics were produced with help of the **ggplot** package. 

For comparison, the learners are ranked on each dataset. (see [benchmark_analysis.R](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/code/benchmark_analysis.R))
There are a few datasets where some of the learners provide errors. 
In the first approach these were treated as having the worst performance and so all learners providing errors get the worst rank. 
If there were several learners they get all the *averaged* worst rank. 

## Classification

The results in the classification case, regarding the accuracy are summarized in the following barplot graphic:

![](/images/2016-08-11-Benchmarking-mlr-learners-on-OpenML/1_best_algo_classif_with_na_rank.png)

It depicts the average rank regarding accuracy over all classification dataset of each learner. 

Clearly the random forest implementations outperform the other. None of the three available packages is clearly better than the other. **svm**, **glmnet** and **cforest** follow. One could probably get better results for **svm** and **xgboost** and some other learners with proper tuning. 

The results for the other measures are quite similar and can be seen [here](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/results/best_algo_classif_rank.pdf). 
In the case of the brier score, **svm** gets the second place and in the logarithmic loss case even the first place. SVM seems to be better suited for these probability measures. 

Regarding training time, **kknn**, **randomForestSRCSyn**, **naiveBayes** and **lda** gets the best results. 

Instead of taking all datasets one could exclude datasets, where some of the learners got errors. The [results](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/results/best_algo_classif_rank.pdf) are quite similar.

## Regression

More interestingly are probably the results of the regression tasks, as there is no available comprehensive regression benchmark study to the best of my knowledge. 

If an algorithm provided an error it was ranked with the worst rank like in the classification case. 

The results for the mean squared error can be seen here:

![](/images/2016-08-11-Benchmarking-mlr-learners-on-OpenML/1_best_algo_regr_with_na_rank.png)

It depicts the average rank regarding mean square error over all regression dataset of each learner. 

Surprisingly the **bartMachine** algorithm performs best! The standard random forest implementations are also all under the top 4.
**cubist**, **glmnet** and **kknn** also perform very good. The standard linear model (**lm**) is "unter ferner liefen". 

[bartMachine](https://arxiv.org/pdf/1312.2171.pdf) and [cubist](https://cran.r-project.org/web/packages/Cubist/vignettes/cubist.pdf) are tree based methods combined with an ensembling method like random forest. 

Once again, if tuning is performed, the ranking would change for algorithms like **svm** and **xgboost**.

Results for the other measures can be seen [here](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/results/best_algo_regr_with_na_rank.pdf).
The average rank of **cubist** gets much better when regarding the mean of absolute error and even gots best, when regarding the median of squared error and median of absolute error. It seems to be a very robust method. 

**kknn** also gets better for the median of squared and absolute error. Regarding the training time it is once again the unbeaten number one. **randomForestSRCSyn** is also much faster than the other random forest implementations. **lm** is also under the best regarding training time. 

When omitting datasets where some of the learners produced errors, only 26 regression datasets remain. **bartMachine** remains best for the mean squared error. The results for the other learners change slightly. See [here](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/results/best_algo_regr_rank.pdf).

<!--chapter:end:2016-08-11-Benchmarking-mlr-learners-on-OpenML.Rmd-->

---
title: "Exploring Learner Predictions with Partial Dependence and Functional ANOVA"
authors: ["zachary-jones"]
date: 2016-08-11
draft: true
categories: ["R", "r-bloggers"]
tags: ["partial dependence", "benchmark", "prediction", "ANOVA", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, eval = FALSE)
```

Learners use features to make predictions but how those features are used is often not apparent.
[mlr](http://github.com/mlr-org/mlr) can estimate the dependence of a learned function on a subset of the feature space using
`generatePartialDependenceData`.

Partial dependence plots reduce the potentially high dimensional function estimated by the learner, and display a marginalized version of this function in a lower dimensional space. For example suppose $\mathbb{E}[Y \ \| \ X = x] = f(x)$. With $(x, y)$ pairs drawn independently, a learner may estimate $\hat{f}$, which, if $X$ is high dimensional can be uninterpretable. Suppose we want to approximate the relationship between some column-wise subset of $X$. We partition $X$ into two sets, $X_s$ and $X_c$ such that $X = X_s \cup X_c$, where $X_s$ is a subset of $X$ of interest.

The partial dependence of $f$ on $X_c$ is

$$f_{X_s} = \mathbb{E}_{X_c}f(X_s, X_c).$$

We can use the following estimator:

$$\hat{f}_{x_s} = \frac{1}{N} \sum_{i = 1}^N \hat{f}(x_s, x_{ic}).$$

This is described by [Friedman (2001)](https://projecteuclid.org/euclid.aos/1013203451) and in [Hastie, Tibsharani, and Friedman (2009)](http://statweb.stanford.edu/~tibs/ElemStatLearn/).

The individual conditional expectation of an observation can also be estimated using the above algorithm absent the averaging, giving $\hat{f}^{(i)}_{x_s}$ as described in [Goldstein, Kapelner, Bleich, and Pitkin (2014)](https://arxiv.org/abs/1309.6392). This allows the discovery of features of $\hat{f}$ that may be obscured by an aggregated summary of $\hat{f}$.

The partial derivative of the partial dependence function, $\frac{\partial \hat f_{x_s}}{\partial x_s}$, and the individual conditional expectation function, $\frac{\partial \hat{f}^{(i)}_{x_s}}{\partial x_s}$, can also be computed. For regression and survival tasks the partial derivative of a single feature $x_s$ is the gradient of the partial dependence function, and for classification tasks where the learner can output class probabilities the Jacobian. Note that if the learner produces discontinuous partial dependence (e.g., piecewise constant functions such as decision trees, ensembles of decision trees, etc.) the derivative will be 0 (where the function is not changing) or trending towards positive or negative infinity (at the discontinuities where the derivative is undefined). Plotting the partial dependence function of such learners may give the impression that the function is not discontinuous because the prediction grid is not composed of all discontinuous points in the predictor space. This results in a line interpolating that makes the function appear to be piecewise linear (where the derivative would be defined except at the boundaries of each piece).

The partial derivative can be informative regarding the additivity of the learned function in certain features. If $\hat{f}^{(i)}_{x_s}$ is an additive function in a feature $x_s$, then its partial derivative will not depend on any other features ($x_c$) that may have been used by the learner. Variation in the estimated partial derivative indicates that there is a region of interaction between $x_s$ and $x_c$ in $\hat{f}$. Similarly, instead of using the mean to estimate the expected value of the function at different values of $x_s$, instead computing the variance can highlight regions of interaction between $x_s$ and $x_c$.

Again, see [Goldstein, Kapelner, Bleich, and Pitkin (2014)](http://arxiv.org/abs/1309.6392) for more details and their package [ICEbox](https://cran.r-project.org/web/packages/ICEbox/index.html) for the original implementation. The algorithm works for any supervised learner with classification, regression, and survival tasks.

## Partial Dependence

Our implementation, following [mlr](http://github.com/mlr-org/mlr)'s [visualization](https://mlr.mlr-org.com/articles/tutorial/visualization.html) pattern, consists
of the above mentioned function `generatePartialDependenceData` and `plotPartialDependence`. The former generates input (objects of class `PartialDependenceData`) for the latter.

The first step executed by `generatePartialDependenceData` is to generate a feature grid for every element of the character vector `features` passed. The data are given by the `input` argument, which can be a `Task` or a `data.frame`. The feature grid can be generated in several ways. A uniformly spaced grid of length `gridsize` (default 10) from the empirical minimum to the empirical maximum is created by default, but arguments `fmin` and `fmax` may be used to override the empirical default (the lengths of `fmin` and `fmax` must match the length of `features`). Alternatively the feature data can be resampled, either by using a bootstrap or by subsampling.

Results from `generatePartialDependenceData` can be visualized with `plotPartialDependence`.

```{r, fig.width = 8, fig.height = 4}
library(mlr)

lrn.classif = makeLearner("classif.ksvm", predict.type = "prob")
fit.classif = train(lrn.classif, iris.task)
pd = generatePartialDependenceData(fit.classif, iris.task, "Petal.Width")
pd

plotPartialDependence(pd, data = iris)
```

As noted above, $x_s$ does not have to be unidimensional. If it is not, the `interaction` flag must be set to `TRUE`. Then the individual feature grids are combined using the Cartesian product, and the estimator above is applied, producing the partial dependence for every combination of unique feature values. If the `interaction` flag is `FALSE` (the default) then by default $x_s$ is assumed unidimensional, and partial dependencies are generated for each feature separately. The resulting output when `interaction = FALSE` has a column for each feature, and `NA` where the feature was not used. With one feature and a regression task the output is a line plot, with a point for each point in the corresponding feature's grid. For classification tasks there is a line for each class (except for binary classification tasks, where the negative class is automatically dropped). The `data` argument to `plotPartialPrediction` allows the training data to be input to show the empirical marginal distribution of the data.


```{r, fig.width = 8, fig.height = 4}
pd.lst = generatePartialDependenceData(fit.classif, iris.task, c("Petal.Width", "Petal.Length"))
head(pd.lst$data)

tail(pd.lst$data)

plotPartialDependence(pd.lst, data = iris)
```

```{r, fig.width = 8, fig.height = 4}
pd.int = generatePartialDependenceData(fit.classif, iris.task, c("Petal.Width", "Petal.Length"), interaction = TRUE)
pd.int

plotPartialDependence(pd.int, facet = "Petal.Length")
```

When `interaction = TRUE`, `plotPartialDependence` can either facet over one feature, showing the conditional relationship between the other feature and $\hat{f}$ in each panel, or a tile plot. The latter is, however, not possible with multiclass classification (an example of a tile plot will be shown later).

At each step in the estimation of $\hat{f}_{x_s}$ a set of predictions of length $N$ is generated. By default the mean prediction is used. For classification where `predict.type = "prob"` this entails the mean class probabilities. However, other summaries of the predictions may be used. For regression and survival tasks the function used here must either return one number or three, and, if the latter, the numbers must be sorted lowest to highest. For classification tasks the function must return a number for each level of the target feature.

As noted, the `fun` argument can be a function which returns three numbers (sorted low to high) for a regression task. This allows further exploration of relative feature importance. If a feature is relatively important, the bounds are necessarily tighter because the feature accounts for more of the variance of the predictions, i.e., it is "used" more by the learner. More directly setting `fun = var` identifies regions of interaction between $x_s$ and $x_c$. This can also be accomplished by computing quantiles. The wider the quantile bounds, the more variation in $\hat{f}$ is due to features other than $x_s$ that is shown in the plot.

```{r}
lrn.regr = makeLearner("regr.ksvm")
fit.regr = train(lrn.regr, bh.task)

pd.ci = generatePartialDependenceData(fit.regr, bh.task, "lstat",
  fun = function(x) quantile(x, c(.25, .5, .75)))
pd.ci

plotPartialDependence(pd.ci)
```

In addition to bounds based on a summary of the distribution of the conditional expectation of each observation, learners which can estimate the variance of their predictions can also be used. The argument `bounds` is a numeric vector of length two which is added (so the first number should be negative) to the point prediction to produce a confidence interval for the partial dependence. The default is the .025 and .975 quantiles of the Gaussian distribution.

```{r}
fit.se = train(makeLearner("regr.randomForest", predict.type = "se"), bh.task)
pd.se = generatePartialDependenceData(fit.se, bh.task, c("lstat", "crim"))
head(pd.se$data)

tail(pd.se$data)

plotPartialDependence(pd.se)
```

As previously mentioned if the aggregation function is not used, i.e., it is the identity, then the conditional expectation of $\hat{f}^{(i)}_{x_s}$ is estimated. If `individual = TRUE` then `generatePartialDependenceData` returns $N$ partial dependence estimates made at each point in the prediction grid constructed from the features.

```{r}
pd.ind.regr = generatePartialDependenceData(fit.regr, bh.task, "lstat", individual = TRUE)
pd.ind.regr

plotPartialDependence(pd.ind.regr)
```

The resulting output, particularly the element `data` in the returned object, has an additional column `idx` which gives the index of the observation to which the row pertains.

For classification tasks this index references both the class and the observation index.

```{r, eval=F}
pd.ind.classif = generatePartialDependenceData(fit.classif, iris.task, "Petal.Length", individual = TRUE)
pd.ind.classif
plotPartialDependence(pd.ind.classif)
```

The plots, at least in these forms, are difficult to interpet. Individual estimates of partial dependence can also be centered by predictions made at all $N$ observations
for a particular point in the prediction grid created by the features. This is controlled by the argument `center` which is a list of the same length as the length of the `features` argument and contains the values of the `features` desired.

```{r, eval=FALSE}
pd.ind.classif = generatePartialDependenceData(fit.classif, iris.task, "Petal.Length", individual = TRUE, center = list("Petal.Length" = min(iris$Petal.Length)))
plotPartialDependence(pd.ind.classif)
```

Partial derivatives can also be computed for individual partial dependence estimates and aggregate partial dependence. This is restricted to a single feature at a time. The derivatives of individual partial dependence estimates can be useful in finding regions of interaction between the feature for which the derivative is estimated and the features excluded. Applied to the aggregated partial dependence function they are not very informative, but when applied to the individual conditional expectations, they can be used to find regions of interaction.

```{r}
pd.regr.der.ind = generatePartialDependenceData(fit.regr, bh.task, "lstat", derivative = TRUE, individual = TRUE)
head(pd.regr.der.ind$data)

plotPartialDependence(pd.regr.der.ind)
```

```{r}
pd.classif.der.ind = generatePartialDependenceData(fit.classif, iris.task, "Petal.Width", derivative = TRUE, individual = TRUE)
head(pd.classif.der.ind$data)

plotPartialDependence(pd.classif.der.ind)
```

This suggests that `Petal.Width` interacts with some other feature in the neighborhood of $(1.5, 2)$ for classes "virginica" and "versicolor".

## Functional ANOVA

[Hooker (2004)](http://dl.acm.org/citation.cfm?id=1014122) proposed the decomposition of a learned function $\hat{f}$ as a sum of lower dimensional functions $$f(\mathbf{x}) = g_0 + \sum_{i = 1}^p g_{i}(x_i) + \sum_{i \neq j} g_{ij}(x_{ij}) + \ldots$$ where $p$ is the number of features. `generateFunctionalANOVAData` estimates the individual $g$ functions using partial dependence. When functions depend only on one feature, they are equivalent to partial dependence, but a $g$ function which depends on more than one feature is the "effect" of only those features: lower dimensional "effects" are removed.

$$\hat{g}_u(x) = \frac{1}{N} \sum_{i = 1}^N \left( \hat{f}(x) - \sum_{v \subset u} g_v(x) \right)$$

Here $u$ is a subset of ${1, \ldots, p}$. When $\|v\| = 1$ $g_v$ can be directly computed by computing the bivariate partial dependence of $\hat{f}$ on $x_u$ and then subtracting off the univariate partial dependences of the features contained in $v$.

Although this decomposition is generalizable to classification it is currently only available for regression tasks.

```{r}
lrn.regr = makeLearner("regr.ksvm")
fit.regr = train(lrn.regr, bh.task)

fa = generateFunctionalANOVAData(fit.regr, bh.task, "lstat", depth = 1, fun = median)
fa

pd.regr = generatePartialDependenceData(fit.regr, bh.task, "lstat", fun = median)
pd.regr
```

The `depth` argument is similar to the `interaction` argument in `generatePartialDependenceData` but instead of specifying whether all of joint "effect" of all the `features` is computed, it determines whether "effects" of all subsets of the features given the specified `depth` are computed. So, for example, with $p$ features and depth 1, the univariate partial dependence is returned. If, instead, `depth = 2`, then all possible bivariate functional ANOVA effects are returned. This is done by computing the univariate partial dependence for each feature and subtracting it from the bivariate partial dependence for each possible pair.

```{r}
fa.bv = generateFunctionalANOVAData(fit.regr, bh.task, c("crim", "lstat", "age"), depth = 2)
fa.bv

names(table(fa.bv$data$effect)) ## interaction effects estimated
```

Plotting univariate and bivariate functional ANOVA components works the same as for partial dependence.

```{r}
fa.bv = generateFunctionalANOVAData(fit.regr, bh.task, c("crim", "lstat"), depth = 2)
plotPartialDependence(fa.bv, geom = "tile", data = getTaskData(bh.task))
```

When overplotting the training data on the plot it is easy to see that much of the variation of the effect is due to extrapolation. Although it hasn't been implemented yet, weighting the functional ANOVA appropriately can ensure that the estimated effects do not depend (or depend less) on regions of the feature space which are sparse.

I also plan on implementing the faster estimation algorith for expanding the functionality of the functional ANOVA function include faster computation using the algorithm in [Hooker (2007)](http://faculty.bscb.cornell.edu/~hooker/fame_jcgs.pdf) and weighting (in order to avoid excessive reliance on points of extrapolation) using outlier detection or joint density estimation.

<!--chapter:end:2016-08-11-exploring-learner-predictions-with-partial-dependence.Rmd-->

---
title: "Result of the mlr summer workshop in Palermo"
author: ["janek-thomas"]
date: 2016-08-15
categories: ["R"]
tags: ["workshop"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

The *mlr developer team* is quite international: Germany, USA, Canada. The time difference between these countries sometimes makes it hard to communicate and develop new features.

The idea for this workshop or [sprint](http://en.wikipedia.org/wiki/Sprint_(software_development)) was to have the possibility to talk about the project status, future and structure, exterminate imperishable bugs and start developing some fancy features.

Since we wanted to meet at a nice place we decided to go to Palermo, where the [department of statistics](https://www.unipa.it/dipartimenti/seas/) provided us with a room in the university for the workshop.

Twelve people from the developer team met from the 8. to 15. August to work on and with mlr.

## Result of the workhop

We closed a lot of issues and developed new features that we will release with version 2.10 of *mlr* in the next few days.

Thanks to all `sample(participants)`: [Giuseppe Casalicchio](http://www.compstat.statistik.uni-muenchen.de/people/casalicchio/), [Janek Thomas](http://www.compstat.statistik.uni-muenchen.de/people/thomas/), [Xudong Sun](http://www.compstat.statistik.uni-muenchen.de/people/Xudong/), [Jakob Bossek](http://www.jakobbossek.de/), [Bernd Bischl](http://www.compstat.statistik.uni-muenchen.de/people/bischl/), [Jakob Richter](http://jakob-r.github.io/), [Michel Lang](https://www.statistik.tu-dortmund.de/lang.html), [Philipp Probst](http://philipppro.github.io/), [Julia Schiffner](https://de.linkedin.com/in/julia-schiffner), [Lars Kotthoff](http://www.cs.ubc.ca/~larsko/), [Zachary Jones](http://zmjones.com/), [Pascal Kerschke](https://www.wi.uni-muenster.de/de/institut/statistik/personen/pascal-kerschke)!

We also head a great time in a great city aside from the workhop, here are some impressions:

![View from Hotel 1](/images/palermo/IMG_20160805_194946.jpg)
![View from Hotel 2](/images/palermo/IMG_20160806_094042.jpg)
![some fountain](/images/palermo/IMG_20160806_112236.jpg)

<!--chapter:end:2016-08-15-The-mlr-workshop.Rmd-->

---
title: "Exploring and Understanding Hyperparameter Tuning"
author: "mason-gallo"
date: '2016-08-21'
tags:
- hyperparameter
- tuning
- rstats
categories:
- R
- r-bloggers
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
set.seed(123)
```

Learners use hyperparameters to achieve better performance on particular 
datasets. When we use a machine learning package to choose the best hyperparmeters,
the relationship between changing the hyperparameter and performance might not 
be obvious. [mlr](http://github.com/mlr-org/mlr) provides several new 
implementations to better understand what happens when we tune hyperparameters 
and to help us optimize our choice of hyperparameters.

# Background

Let's say you have a dataset, and you're getting ready to flex your machine 
learning muscles. Maybe you want to do classification, or regression, or 
clustering. You get your dataset together and pick a few learners to evaluate. 

The majority of learners that you might use for any of these tasks 
have hyperparameters that the user must tune. Hyperparameters may be able to take 
on a lot of possible values, so it's typically left to the user to specify the 
values. If you're using a popular machine learning library like [sci-kit learn](http://scikit-learn.org/), 
the library will take care of this for you via cross-validation: auto-generating 
the optimal values for your hyperparameters. We'll then take these best-performing 
hyperparameters and use those values for our learner. Essentially, we treat the 
optimization of hyperparameters as a black box. 

In [mlr](http://github.com/mlr-org/mlr), we want to open up that black box, so 
that you can make better decisions. Using the functionality built-in, we can 
answer questions like:

- How does varying the value of a hyperparameter change the performance of the machine learning algorithm?
- On a related note: where's an ideal range to search for optimal hyperparameters?
- How did the optimization algorithm (prematurely) converge?
- What's the relative importance of each hyperparameter?

Some of the users who might see benefit from "opening" the black box of hyperparameter 
optimization:

- researchers that want to better understand learners in practice
- engineers that want to maximize performance or minimize run time
- teachers that want to demonstrate what happens when tuning hyperparameters

We'll use [Pima Indians](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) dataset in this blog post, where we want to 
predict whether or not someone has diabetes, so we'll perform classification, 
but the methods we discuss also work for regression and clustering.

Perhaps we decide we want to try [kernlab's svm](http://www.rdocumentation.org/packages/kernlab/versions/0.9-24) for our 
classification task. Knowing that svm has several hyperparameters to tune, we 
can ask mlr to list the hyperparameters to refresh our memory:

```{r, message = FALSE, warning = FALSE}
library(mlr)
library(ggplot2)
# to make sure our results are replicable we set the seed
set.seed(7)
getParamSet("classif.ksvm")
```

Noting that we have default values for each of the hyperparameters, we could 
simply accept the defaults for each of the hyperparameters and evaluate our 
`mmce` performance using 3-fold cross validation:

```{r, message = FALSE, warning = FALSE}
rdesc = makeResampleDesc("CV", iters = 3)
r = resample("classif.ksvm", pid.task, rdesc)
print(r)
```

While this result may seem decent, we have a nagging doubt: what if we chose 
hyperparameter values different from the defaults? Would we get better results?

Maybe we believe that the default of `kernel = "rbfdot"` will work well based 
on our prior knowledge of the dataset, but we want to try altering our 
regularization to get better performance. For [kernlab's svm](http://www.rdocumentation.org/packages/kernlab/versions/0.9-24), regularization 
is represented using the `C` hyperparameter. Calling `getParamSet` again to 
refresh our memory, we see that `C` defaults to 1.

```{r, message = FALSE, warning = FALSE}
getParamSet("classif.ksvm")
```

Let's tell [mlr](http://github.com/mlr-org/mlr) to randomly pick `C` values 
between `2^-5` and `2^5`, evaluating `mmce` using 3-fold cross validation:

```{r, message = FALSE, warning = FALSE}
# create the C parameter in continuous space: 2^-5 : 2^5
ps = makeParamSet(
  makeNumericParam("C", lower = -5, upper = 5, trafo = function(x) 2^x)
)
# random search in the space with 100 iterations
ctrl = makeTuneControlRandom(maxit = 100L)
# 3-fold CV
rdesc = makeResampleDesc("CV", iters = 2L)
# run the hyperparameter tuning process
res = tuneParams("classif.ksvm", task = pid.task, control = ctrl, 
  resampling = rdesc, par.set = ps, show.info = FALSE)
print(res)
```

[mlr](http://github.com/mlr-org/mlr) gives us the best performing value for `C`, 
and we can see that we've improved our results vs. just accepting the default 
value for `C`. This functionality is available in other machine learning packages, like 
sci-kit learn's [random search](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.RandomizedSearchCV.html), but this functionality is essentially treating our
choice of `C` as a black box method: we give a search strategy and just accept 
the optimal value. What if we wanted to get a sense of the relationship between 
`C` and `mmce`? Maybe the relationship is linear in a certain range and we can 
exploit this to get better even performance! [mlr](http://github.com/mlr-org/mlr) 
provides 2 methods to help answer this question: `generateHyperParsEffectData` to 
generate the resulting data and `plotHyperParsEffect` providing many options 
built-in for the user to plot the data. 

Let's investigate the results from before where we tuned `C`:

```{r first_chart, message = FALSE, warning = FALSE}
data = generateHyperParsEffectData(res)
plotHyperParsEffect(data, x = "C", y = "mmce.test.mean")
```

From the scatterplot, it appears our optimal performance is somewhere in the 
region between `2^-2.5` and `2^-1.75`. This could provide us a region to further 
explore if we wanted to try to get even better performance!

We could also evaluate how "long" it takes us to find that optimal value:

```{r second_chart, message = FALSE, warning = FALSE}
plotHyperParsEffect(data, x = "iteration", y = "mmce.test.mean")
```

By default, the plot only shows the global optimum, so we can see that we found 
the "best" performance in less than 25 iterations!

But wait, I hear you saying. I also want to tune `sigma`, the inverse kernel 
width of the radial basis kernel function. So now we have 2 hyperparameters that 
we want to simultaneously tune: `C` and `sigma`. 

```{r, message = FALSE, warning = FALSE}
# create the C and sigma parameter in continuous space: 2^-5 : 2^5
ps = makeParamSet(
  makeNumericParam("C", lower = -5, upper = 5, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -5, upper = 5, trafo = function(x) 2^x)
)
# random search in the space with 100 iterations
ctrl = makeTuneControlRandom(maxit = 100L)
# 3-fold CV
rdesc = makeResampleDesc("CV", iters = 2L)
# run the hyperparameter tuning process
res = tuneParams("classif.ksvm", task = pid.task, control = ctrl, 
  resampling = rdesc, par.set = ps, show.info = FALSE)
print(res)
# collect the hyperparameter data
data = generateHyperParsEffectData(res)
```

We can use `plotHyperParsEffect` to easily create a heatmap with both hyperparameters. 
We get tons of functionality for free here. For example, [mlr](http://github.com/mlr-org/mlr) 
will automatically interpolate the grid to get an estimate for values we didn't 
even test! All we need to do is pass a regression learner to the `interpolate` 
argument:

```{r third_chart, message = FALSE, warning = FALSE}
plotHyperParsEffect(data, x = "C", y = "sigma", z = "mmce.test.mean",
  plot.type = "heatmap", interpolate = "regr.earth")
```

If we use the `show.experiments` argument, we can see which points were 
actually tested and which were interpolated:

```{r fourth_chart, message = FALSE, warning = FALSE}
plotHyperParsEffect(data, x = "C", y = "sigma", z = "mmce.test.mean",
  plot.type = "heatmap", interpolate = "regr.earth", 
  show.experiments = TRUE)
```

`plotHyperParsEffect` returns a `ggplot2` object, so we can always customize it 
to better fit our needs downstream:

```{r fifth_chart, message = FALSE, warning = FALSE}
plt = plotHyperParsEffect(data, x = "C", y = "sigma", z = "mmce.test.mean",
  plot.type = "heatmap", interpolate = "regr.earth", 
  show.experiments = TRUE)
min_plt = min(plt$data$mmce.test.mean, na.rm = TRUE)
max_plt = max(plt$data$mmce.test.mean, na.rm = TRUE)
mean_plt = mean(c(min_plt, max_plt))
plt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 4),
  low = "red", mid = "white", high = "blue", midpoint = mean_plt)
```

Now we can get a good sense of where the separation happens for each of the 
hyperparameters: in this particular example, we want lower values for `sigma` 
and values around 1 for `C`.

This was just a taste of mlr's hyperparameter tuning visualization capabilities. For the full tutorial, check out the [mlr tutorial](https://mlr.mlr-org.com/articles/tutorial/devel/hyperpar_tuning_effects.html).

Some features coming soon:

- "Prettier" plot defaults
- Support for more than 2 hyperparameters
- Direct support for hyperparameter "importance"

Thanks to the generous sponsorship from [GSoC](https://summerofcode.withgoogle.com/), and many thanks to my mentors Bernd Bischl and Lars Kotthoff!

<!--chapter:end:2016-08-21-Exploring-and-Understanding-Hyperparameter-Tuning.Rmd-->

---
title: "How to win a drone in 20 lines of R code"
authors: ["janek-thomas"]
date: 2016-08-23
categories: ["R", "r-bloggers"]
tags: ["kaggle", "xgboost", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = F)
```

Or a less clickbaity title: __Model based optimization of machine learning models with [mlr](https://github.com/mlr-org/mlr) and [mlrMBO](https://github.com/mlr-org/mlrMBO)__.

I recently participated in the [#TEFDataChallenge](http://www.tefdatachallenge.com/) a _datathon_ organized by Wayra. 
The first price was a drone for every team member, which is a pretty awesome price.

So what exactly is a datathon?

> At a datathon multiple teams consisting out of computer engineers, data scientists and experts from other fields come together to work on a case for 24 hours straight, where the purpose is to study and analyze data related to the case. Methods well known in AI will be used. A datathon is derived from a so-called hackathon, where in this case the focus is more on data rather than on innovation. Although with the upcoming field of big data bigger and more complicated problems can be solved, there is still a lack of real data scientists. Therefore a datathon is the solution: In a fun and challenging way people are triggered to come up with the best ideas! The Team with the most outstanding patterns or the best predictive models will win the datathon!

[Source](http://datathon.xomnia.com/what-is-a-datathon)

I already took part in some hackathons, but never in a datathon so this was a bit new for me. But
only 24 hours to create a prediction model as well as an interesting visualization seems quite hard.

Since I signed a NDA, I won't talk about the data used in the challenge. Only so much (that's the information you can find on their website): 

> The aim is to build a prediction model for customer churn, a model to predict if a customer will or will not cancel a mobile contract. 

This is a typical binary classification problem. Fortunately the data was already nicely preprocessed.
No missing values or data garbage, they even aggregated the data set already so that only one observation per costumer remained.
This makes the problem a bit easier: _find the best prediction model in a short amount of time_, but
most importantly: We don't want to spend hours coding it, so that we can use most of our time to 
create an awesome visualization and create/scrape additional features.

Typically in these kind of challenges [stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking) results in the winning model. But this problematic in this case because

a) Stacking multiple models takes an extremely long time and/or huge computation power. We only had one
EC2 instance.

b) The judges wanted to have simple models, the simpler the model the better, while still having extremely good prediction accuracy.

So what to do when we cannot stack a hugely complected model ensemble? We use gradient boosting with trees.
This has multiple advantages, we have a (rather) simple model with good prediction accuracy, we can handle
categorical variables with large number of classes and we get variable importance measures which can be
used in the visualization of the model. (We can also create [partial dependence plots](https://mlr-org.github.io/exploring-learner-predictions-with-partial-dependence/) to gain even more insights in the effects). We use [xgboost](https://github.com/dmlc/xgboost) which is currently the fastest implementation for gradient boosting with trees.

Here is our learner definition with the param-set we want to optimize.   
```{r, eval=FALSE}
library(mlr)
lrn = makeLearner("classif.xgboost", eval_metric = "auc", 
  predict.type = "prob")

ps = makeParamSet(
  makeIntegerParam("nrounds", lower = 200, upper = 2500, default = 200),
  makeNumericParam("eta", lower = -7, upper = -5, default = -6, 
    trafo = function(x) 2^x),
  makeIntegerParam("max_depth", lower = 3, upper = 15, default = 3),
  makeNumericParam("colsample_bytree", lower = 0.3, upper = 1, 
   default = 0.6),
  makeNumericParam("subsample", lower = 0.3, upper = 1, default = 0.6)
)
```

Overall we have 5 parameters (number of trees, learning rate, tree depth, bagging fraction and sub-sampling fraction) that we vary to find our optimal model.
For standard optimization techniques like grid search this is already a real problem because the numbers of points to search increases exponentially with every additional hyper-parameter.

Since all hyper-parameter are numeric it would be possible to use evolutionary algorithms like [cmaes](https://en.wikipedia.org/wiki/CMA-ES).
The problem with that is, that these methods generally take a large number of function evaluation to find the optimal model, but we don't have too much time and (cross-validated) model fits are quite expensive.

This is basically the perfect situation for model based optimization. We fit a surrogate model over
the space of hyper-parameters and search promising points. Having only numeric hyper-parameter makes the optimization 
even nicer because we can use a Gaussian process as our surrogate model. In the figure you can see an example of model based optimization in 1 dimension. The upper figure shows evaluated points, the estimated model (dashed line) and its variance (gray area). The lower picture shows how interesting every possible point is for future exploration of the space. 
For an in-depth introduction to model-based optimization you can read [Jones(1998)](http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/$FILE/Jones98.pdf).

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
library(smoof)
library(mlr)
library(mlrMBO)
library(ParamHelpers)
obj.fun = makeAckleyFunction(1)
learner = makeLearner("regr.km", predict.type = "se", covtype = "matern5_2", control = list(trace = FALSE))
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 10)
control = setMBOControlInfill(control, crit = crit.ei)
set.seed(12)
ex = exampleRun(obj.fun, control = control, show.info = FALSE, learner = learner)
```

```{r, eval=TRUE, echo=FALSE, fig.align="center", message=FALSE}
plotExampleRun(ex, 1L)
```

We use [mlrMBO](https://github.com/mlr-org/mlrMBO) as a general black-box optimization toolkit with is already nicely connected to mlr. The optimization definition looks like this:

```{r, eval=FALSE}
library(mlrMBO)
library(parallelMap)
task = makeClassifTask(data = data, target = "churn")
mbo.ctrl = makeMBOControl(save.on.disk.at = c(0, 5, 10, 20, 50, 75, 85, 95))
mbo.ctrl = setMBOControlTermination(mbo.ctrl, iters = 100)
surrogate.lrn = makeLearner("regr.km", predict.type = "se")
ctrl = mlr:::makeTuneControlMBO(learner = surrogate.lrn, 
                                mbo.control = mbo.ctrl)

parallelStartMulticore(cpus = 10L)
res.mbo = tuneParams(lrn, task, cv10, par.set = ps, control = ctrl, 
  show.info = TRUE, measures = auc)
parallelStop()
```

Some notes on this:

- We save our model (actually the optimization path) at different iterations of the process, which is quite useful if anything crashes.
- We do 100 sequential iterations after the initial design (for which we used the default setting).
- We use kriging as our surrogate model. Actually we don't even need to specify this, since it is the default surrogate model of [mlrMBO](https://github.com/mlr-org/mlrMBO).
- [parallelMap](https://github.com/berndbischl/parallelMap) is used to parallelize the cross-validation over all 10 available cores on the EC2 instance.

The resulting model was in the end the best model on the hidden test set and even beat a large stacking ensemble of
multiple boosting models, random forests and deep neural networks.

One other team used a quite similar approach, but instead of model based optimization they used [irace](http://iridia.ulb.ac.be/irace/) to tune a gradient boosting model. This can also be done in mlr quite easily:

```{r, eval=FALSE}
ctrl = makeTuneControlIrace(n.instances = 200L)
parallelStartMulticore(cpus = 10L)
res.irace = tuneParams(lrn, task, cv10, par.set = ps, control = ctrl, 
  show.info = TRUE, measures = auc)
parallelStop()
```

<!--chapter:end:2016-08-23-How-to-win-a-drone-in-20-lines-of-R-code.Rmd-->

---
title: "mlr loves OpenML"
authors: ["heidi-seibold"]
date: 2016-09-09
categories: ["R", "r-bloggers"]
tags: ["OpenML", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

[OpenML](http://www.openml.org/) stands for Open Machine Learning and is an
online platform, which aims at supporting collaborative machine learning
online. It is an Open Science project that allows its users to share data, code
and machine learning experiments.

At the time of writing this blog I am in Eindoven at an [OpenML
workshop](http://openml2016dev.openml.org/), where developers and scientists
meet to work on improving the project. Some of these people are R users and they (we)
are developing an [R package](https://github.com/openml/openml-r) that
communicates with the OpenML platform.

![](/images/2016-09-09-mlr-loves-OpenML/mlr_loves_openml.png) 

## OpenML in R

The OpenML R package can list and download data sets and machine
learning tasks (prediction challenges).  In R one can run algorithms on the
these data sets/tasks and
then upload the results to OpenML. After successful uploading, the website shows how well the
algorithm performs.  To run the algorithm on a given task the OpenML R package
builds on the [mlr](https://github.com/mlr-org/mlr) package. mlr understands
what a task is and can run learners on that task. So all the OpenML package
needs to do is convert the OpenML objects to objects mlr understands and then
mlr deals with the learning.

## A small case study

We want to create a little study on the [OpenML
website](http://www.openml.org/), in which we compare different types of Support
Vector Machines.  The study gets an ID assigned to it, which in our case is 27.
We use the function ksvm (with different settings of the function argument type)
from package kernlab, which is integrated in mlr ("classif.ksvm").

![](/images/2016-09-09-mlr-loves-OpenML/openml_screenshot_study.png) 

For details on installing and setting up the OpenML R package please see the
[guide](https://github.com/openml/openml-r) on GitHub.

Let's start conducting the study:

- Load the packages and list all tasks which have between 100 and 500
  observations. 
  
```{r, eval=TRUE, message=FALSE}
library("OpenML")
library("mlr")
library("farff")
library("BBmisc")
```

```{r}
dsize = c(100, 500)
taskinfo_all = listOMLTasks(number.of.instances = dsize)
```

- Select all supervised classification tasks that do 10-fold cross-validation
  and choose only one task per data set. To keep the study simple and fast to compute, 
  select only the first three tasks.
  
```{r}
taskinfo_10cv = subset(taskinfo_all, task.type == "Supervised Classification" & 
                    estimation.procedure == "10-fold Crossvalidation" &
                    evaluation.measures == "predictive_accuracy" &
                    number.of.missing.values == 0 &
                    number.of.classes %in% c(2, 4))
taskinfo = taskinfo_10cv[1:3, ]
```

- Create the learners we want to compare.

```{r}
lrn.list = list(
  makeLearner("classif.ksvm", type = "C-svc"),
  makeLearner("classif.ksvm", type = "kbb-svc"),
  makeLearner("classif.ksvm", type = "spoc-svc")
)
```

- Run the learners on the three tasks.

```{r}
grid = expand.grid(task.id = taskinfo$task.id, 
                   lrn.ind = seq_along(lrn.list))

runs = lapply(seq_row(grid), function(i) {
  message(i)
  task = getOMLTask(grid$task.id[i])
  ind = grid$lrn.ind[i]
  runTaskMlr(task, lrn.list[[ind]])
})
```

- And finally upload the runs to OpenML.  The upload function (uploadOMLRun)
  returns the ID of the uploaded run object.  When uploading runs that are part
of a certain study, tag it with study_ and the study ID. After uploading the runs appear
on the website and can be found using the tag or via the
[study homepage](http://www.openml.org/index.php/s/27).

```{r, eval=FALSE}
## please do not spam the OpenML server by uploading these
## tasks. I already did that.
run.id = lapply(runs, uploadOMLRun, tags = "study_27")
```

To show the results of our study, list the run evaluations and make a nice plot.

```{r, eval=TRUE} 
evals = listOMLRunEvaluations(tag = "study_27")

evals$task.id = as.factor(evals$task.id)
evals$setup.id = as.factor(evals$setup.id)

library("ggplot2")
ggplot(evals, aes(x = setup.id, y = predictive.accuracy, 
                  color = data.name, group = task.id)) + 
  geom_point() + geom_line()
```

Now you can go ahead and create a bigger study using the techniques you have learned.

## Further infos

If you are interested in more, check out the OpenML
[blog](https://medium.com/open-machine-learning), the
[paper](https://www.researchgate.net/publication/263890323_OpenML_Networked_science_in_machine_learning)
and the [GitHub repos](https://github.com/openml).

<!--chapter:end:2016-09-09-mlr-loves-OpenML.Rmd-->

---
title: "Paper published: mlr - Machine Learning in R"
authors: ["jakob-richter"]
date: 2016-10-20
categories: ["R", "r-bloggers"]
tags: ["JMLR", "paper", "science", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

We are happy to announce that we can finally answer the question on how to cite *mlr* properly in publications.

Our paper on *mlr* has been published in the open-access Journal of Machine Learning Research (JMLR) and can be downloaded on the [journal home page](http://www.jmlr.org/papers/v17/15-066.html).

[![](/images/2016-10-20-Paper-published-mlr-Machine-Learning-in-R/paper-first-page.png)](http://www.jmlr.org/papers/v17/15-066.html)

The paper gives a brief overview of the features of *mlr* and also includes a comparison with similar toolkits.
For an in-depth understanding we still recommend our excellent [online mlr tutorial](https://mlr.mlr-org.com/) which is now also available as a [PDF](https://arxiv.org/abs/1609.06146) on arxiv.org or as [zipped HTML files](https://mlr.mlr-org.com/devel/mlr_tutorial.zip) for offline reading.

Once *mlr 2.10* hits CRAN you can retrieve the citation information from within R:

```{r, eval=FALSE}
citation("mlr")
```

```{r, echo=FALSE}
tmp = capture.output(citation("mlr"))
tmp = stringi::stri_replace_all_fixed(tmp, "{{", "{% raw %}{{{% endraw %}")
cat(paste0(tmp, "\n"))
```

<!--chapter:end:2016-10-20-Paper-published-mlr-Machine-Learning-in-R.Rmd-->

---
title: "mlr 2.10"
authors: ["janek-thomas"]
date: 2017-02-13
categories: ["R"]
tags: ["mlr", "changelog"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

mlr 2.10 is now on CRAN. Please update your package if you haven't done so in a while.

Here is an overview of the changes:

## functions - general
* fixed bug in resample when using predict = "train" (issue #1284)
* update to irace 2.0 -- there are algorithmic changes in irace that may affect
  performance
* generateFilterValuesData: fixed a bug wrt feature ordering
* imputeLearner: fixed a bug when data actually contained no NAs
* print.Learner: if a learner hyperpar was set to value "NA" this was not
  displayed in printer
* makeLearner, setHyperPars: if you mistype a learner or hyperpar name, mlr
  uses fuzzy matching to suggest the 3 closest names in the message
* tuneParams: tuning with irace is now also parallelized, i.e., different
  learner configs are evaluated in parallel.
* benchmark: mini fix, arg 'learners' now also accepts class strings
* object printers: some mlr printers show head previews of data.frames.
  these now also print info on the total nr of rows and cols and are less confusing
* aggregations: have better properties now, they know whether they require training or
  test set evals
* the filter methods have better R docs
* filter randomForestSRC.var.select: new arg "method"
* filter mrmr: fixed some smaller bugs and updated properties
* generateLearningCurveData: also accepts single learner, does not require a list
* plotThreshVsPerf: added "measures" arg
* plotPartialDependence: can create tile plots with joint partial dependence
  on two features for multiclass classification by facetting across the classes
* generatePartialDependenceData and generateFunctionalANOVAData: expanded
  "fun" argument to allow for calculation of weights
* new "?mlrFamilies" manual page which lists all families and the functions
  belonging to it
* we are converging on data.table as a standard internally, this should not
  change any API behavior on the outside, though
* generateHyperParsEffectData and plotHyperParsEffect now support more than 2
  hyperparameters
* linear.correlation, rank.correlation, anova.test: use Rfast instead of
  FSelector/custom implementation now, performance should be much better
* use of our own colAUC function instead of the ROCR package for AUC calculation
  to improve performance
* we output resample performance messages for every iteration now
* performance improvements for the auc measure
* createDummyFeatures supports vectors now
* removed the pretty.names argument from plotHyperParsEffect -- labels can be set
  though normal ggplot2 functions on the returned object
* Fixed a bad bug in resample, the slot "runtime" or a ResampleResult,
  when the runtime was measured not in seconds but e.g. mins. R measures then potentially in mins,
  but mlr claimed it would be seconds.
* New "dummy" learners (that disregard features completely) can be fitted now for baseline comparisons,
  see "featureless" learners below.

## functions - new
* filter: randomForest.importance
* generateFeatureImportanceData: permutation-based feature importance and local
  importance
* getFeatureImportanceLearner: new Learner API function
* getFeatureImportance: top level function to extract feature importance
  information
* calculateROCMeasures
* calculateConfusionMatrix: new confusion-matrix like function that calculates
  and tables many receiver operator measures
* makeLearners: create multiple learners at once
* getLearnerId, getLearnerType, getLearnerPredictType, getLearnerPackages
* getLearnerParamSet, getLearnerParVals
* getRRPredictionList
* addRRMeasure
* plotResiduals
* getLearnerShortName
* mergeBenchmarkResults

## functions - renamed
* Renamed rf.importance filter (now deprecated) to randomForestSRC.var.rfsrc
* Renamed rf.min.depth filter (now deprecated) to randomForestSRC.var.select
* Renamed getConfMatrix (now deprecated) to calculateConfusionMatrix
* Renamed setId (now deprecated) to setLearnerId

## functions - removed
* mergeBenchmarkResultLearner, mergeBenchmarkResultTask

## learners - general
* classif.ada: fixed some param problem with rpart.control params
* classif.cforest, regr.cforest, surv.cforest:
  removed parameters "minprob", "pvalue", "randomsplits"
  as these are set internally and cannot be changed by the user
* regr.GPfit: some more params for correlation kernel
* classif.xgboost, regr.xgboost: can now properly handle NAs (property was missing and other problems), added "colsample_bylevel" parameter
* adapted {classif,regr,surv}.ranger parameters for new ranger version

## learners - new
* multilabel.cforest
* surv.gbm
* regr.cvglmnet
* {classif,regr,surv}.gamboost
* classif.earth
* {classif,regr}.evtree
* {classif,regr}.evtree

## learners - removed
* classif.randomForestSRCSyn, regr.randomForestSRCSyn: due to continued stability issues

## measures - new
* ssr, qsr, lsr
* rrse, rae, mape
* kappa, wkappa
* msle, rmsle

<!--chapter:end:2017-02-13-mlr-210.Rmd-->

---
title: "mlr Google Summer of Code 2017"
authors: ["janek-thomas"]
date: 2017-02-13
categories: ["R"]
tags: ["GSCO", "mlr"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

We are happy to announce that we applied for a another [Google Summer of Code](https://developers.google.com/open-source/gsoc/) project in 2017.

[Operator Based Machine Learning Pipeline Construction](https://github.com/rstats-gsoc/gsoc2017/wiki/Operator-Based-Machine-Learning-Pipeline-Construction)

We aim to change the way we are currently doing data preprocessing in mlr. Have a look at the proposal linked above for more details.

If you are interested in doing this project, have a look at the tests and requirements.

<!--chapter:end:2017-02-13-mlr-GSOC.Rmd-->

---
title: "mlr Workshop 2017"
authors: ["janek-thomas"]
date: 2017-02-13
categories: ["R"]
tags: ["workshop", "tutorial"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

## When and Where?

In 2017, we are hosting the workshop at LMU Munich. The workshop will run from 6 March to 10 March 2017 (potentially including the sunday before and the saturday at the end), hosted by the Ludwig-Maximilians-University Munich.

*Important Dates:*

* Address: [Geschwister-Scholl-Platz 1](https://goo.gl/maps/qZtJHsjQcMw), Room: M203.
* Start: 6th of March: 10:00 AM.

**It is also possible to arrive on Saturday or Sunday, as we already have the rooms and are able to work there. But this is totally optional and the official workshop starts on Monday. Same thing for the Saturday after the workshop.**

## Why? 

The mlr developer team is quite international: Germany, USA, Canada. The time difference between these countries sometimes makes it hard to communicate and develop new features. The idea for this workshop or sprint was to have the possibility to talk about the project status, future and structure, exterminate imperishable bugs and start developing some fancy features.

## What is the target audience / Can I join?

The workshop is mainly geared towards the already existing crowd of developers, but it might also be a perfect opportunity to join the team - if you want to help. We are always looking for competent persons to collaborate with. 
If you are interested, please register in the following form, and we are looking forward to seeing you in Munich! Join us for the excellent team and the nice Bavarian beer and food in Munich!


<a class="typeform-share button" href="https://janek4.typeform.com/to/zCUYc8" data-mode="1" target="_blank">Register here</a>
<script>(function(){var qs,js,q,s,d=document,gi=d.getElementById,ce=d.createElement,gt=d.getElementsByTagName,id='typef_orm',b='https://s3-eu-west-1.amazonaws.com/share.typeform.com/';if(!gi.call(d,id)){js=ce.call(d,'script');js.id=id;js.src=b+'share.js';q=gt.call(d,'script')[0];q.parentNode.insertBefore(js,q)}id=id+'_';if(!gi.call(d,id)){qs=ce.call(d,'link');qs.rel='stylesheet';qs.id=id;qs.href=b+'share-button.css';s=gt.call(d,'head')[0];s.appendChild(qs,s)}})()</script>

<br>

## Sponsors

We want to thank all our sponsors. Without them this workshop would not be possible.

![](/images/2017-02-13-mlr-workshop/accenture.PNG "Accenture")
![](/images/2017-02-13-mlr-workshop/alexanderthamm.png)
![](/images/2017-02-13-mlr-workshop/bosch.png)
![](/images/2017-02-13-mlr-workshop/burdaforward.png)


## Timetable and schedule

You can find the workshop schedule [here](https://docs.google.com/spreadsheets/d/1lsdcxIdoz1acxwKScXjmhNmGgQyH2dDkN04h1HRxAYo/edit?usp=sharing)

## Results of the previous workshop

Our last workshop in 2016 was in Palermo, Italy. Twelve people from the developer team met from the 8th to 15th of August to work on and with mlr and it was more like a sprint, as our core developers meet to get stuff done. We closed a lot of issues and developed new features that we will release with version 2.10 of mlr. 
Thanks to all participants: Giuseppe Casalicchio, Janek Thomas, Xudong Sun, Jakob Bossek, Bernd Bischl, Jakob Richter, Michel Lang, Philipp Probst, Julia Schiffner, Lars Kotthoff, Zachary Jones, Pascal Kerschke!
We also head a great time in a great city aside from the workshop, where we have been travelling around the city for sightseeing and enjoying the beach and nice food of Palermo.

<!--chapter:end:2017-02-13-mlr-workshop.Rmd-->

---
title: "OpenML tutorial at useR!2017 Brussels"
authors: ["giuseppe-casalicchio"]
date: 2017-03-02
categories: ["R", "r-bloggers"]
tags: ["OpenML", "tutorial", "useR"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

## What is OpenML?

Conducting research openly and reproducibly is becoming the gold standard in academic research. Practicing open and reproducible research, however, is hard. [OpenML.org](https://www.openml.org) (Open Machine Learning) is an online platform that aims at making the part of research involving data and analyses easier. It automatically connects data sets, research tasks, algorithms, analyses and results and allows users to access all components including meta information through a REST API in a machine readable and standardized format. Everyone can see, work with and expand other people’s work in a fully reproducible way. 

## The useR Tutorial

At [useR!2017](https://user2017.brussels/), we will we will present an [R package](https://github.com/openml/r) to interface the OpenML platform and illustrate its usage both as a stand-alone package and in combination with the [mlr](https://github.com/mlr-org/mlr) machine learning package. Furthermore, we show how the OpenML package allows R users to easily search, download and upload machine learning datasets.

<!--chapter:end:2017-03-02-OpenML-tutorial-at-useR.Rmd-->

---
title: "Being successful on Kaggle using mlr"
authors: ["giuseppe-casalicchio"]
date: 2017-03-09
categories: ["R", "r-bloggers"]
tags: ["kaggle", "mlr", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

Achieving a good score on a Kaggle competition is typically quite difficult. 
This blog post outlines 7 tips for beginners to improve their ranking on the Kaggle leaderboards. 
For this purpose, I also created a [*Kernel*](https://www.kaggle.com/casalicchio/bike-sharing-demand/tuning-with-mlr) 
for the [*Kaggle bike sharing competition*](https://www.kaggle.com/c/bike-sharing-demand) 
that shows how the R package, `mlr`, can be used to tune a xgboost model with random search in parallel (using 16 cores). The R script scores rank 90 (of 3251) on the Kaggle leaderboard.

## 7 Rules

  1. Use good software
  2. Understand the objective
  3. Create and select features
  4. Tune your model
  5. Validate your model
  6. Ensemble different models
  7. Track your progress

### 1. Use good software

Whether you choose R, Python or another language to work on Kaggle, you will 
most likely need to leverage quite a few packages to follow best practices in 
machine learning. To save time, you should use 'software'
that offers a standardized and well-tested interface for the important steps 
in your workflow:

  - Benchmarking different machine learning algorithms (learners)
  - Optimizing hyperparameters of learners
  - Feature selection, feature engineering and dealing with missing values
  - Resampling methods for validation of learner performance
  - Parallelizing the points above
  
Examples of 'software' that implement the steps above and more: 

  - For python: scikit-learn (<http://scikit-learn.org/stable/auto_examples>).
  - For R: `mlr` (<https://mlr.mlr-org.com/index.html>) or `caret`.


### 2. Understand the objective
  
To develop a good understanding of the Kaggle challenge, you should:

  - Understand the problem domain:
    - Read the description and try to understand the aim of the competition. 
    - Keep reading the forum and looking into scripts/kernels of others, learn from them!
    - Domain knowledge might help you (i.e., read publications about the topic, wikipedia is also ok).
    - Use external data if allowed (e.g., google trends, historical weather data).
    
  - Explore the dataset:
    - Which features are numerical, categorical, ordinal or time dependent?
    - Decide how to handle [*missing values*](https://mlr.mlr-org.com/articles/tutorial/impute.html). Some options:
        - Impute missing values with the mean, median or with values that are out of range (for numerical features).
        - Interpolate missing values if the feature is time dependent.
        - Introduce a new category for the missing values or use the mode (for categorical features).
    - Do exploratory data analysis (for the lazy: wait until someone else uploads an EDA kernel). 
    - Insights you learn here will inform the rest of your workflow (creating new features).
    
Make sure you choose an approach that directly optimizes the measure of interest!
Example: 

  - The **median** minimizes the mean absolute error **(MAE)** and 
  the **mean** minimizes the mean squared error **(MSE)**. 
  - By default, many regression algorithms predict the expected **mean** but there 
  are counterparts that predict the expected **median** 
  (e.g., linear regression vs. quantile regression).
  <!-- - Some measures use a (log-)transformation of the target  -->
  <!-- (e.g. the **RMSLE**, see [*bike sharing competition*](https://www.kaggle.com/c/bike-sharing-demand/details/evaluation)). \newline -->
  <!-- $\rightarrow$ transform the target in the same way before modeling. -->
  - For strange measures: Use algorithms where you can implement your own objective 
  function, see e.g. 
      - [*tuning parameters of a custom objective*](https://www.kaggle.com/casalicchio/allstate-claims-severity/tuning-the-parameter-of-a-custom-objective-1120) or 
      - [*customize loss function, and evaluation metric*](https://github.com/tqchen/xgboost/tree/master/demo#features-walkthrough).


### 3. Create and select features:

In many kaggle competitions, finding a "magic feature" can dramatically increase your ranking.
Sometimes, better data beats better algorithms!
You should therefore try to introduce new features containing valuable information 
(which can't be found by the model) or remove noisy features (which can decrease model performance):

  - Concat several columns
  - Multiply/Add several numerical columns
  - Count NAs per row
  - Create dummy features from factor columns
  -  For time series, you could try
      - to add the weekday as new feature
      - to use rolling mean or median of any other numerical feature
      - to add features with a lag...
  - Remove noisy features: [*Feature selection / filtering*](https://mlr.mlr-org.com/articles/tutorial/feature_selection.html)

### 4. Tune your model

Typically you can focus on a single model (e.g. [*xgboost*](https://xgboost.readthedocs.io/en/latest)) and tune its hyperparameters for optimal performance.

  - Aim: 
  Find the best hyperparameters that, for the given data set, optimize the pre-defined performance measure.
  - Problem: 
  Some models have many hyperparameters that can be tuned.
  - Possible solutions: 
    - [*Grid search or random search*](https://mlr.mlr-org.com/articles/tutorial/devel/tune.html)
    - Advanced procedures such as [*irace*](https://mlr.mlr-org.com/articles/tutorial/devel/advanced_tune.html) 
    or [*mbo (bayesian optimization)*](https://mlrMBO.mlr-org.com/articles/mlrMBO.html)

### 5. Validate your model

Good machine learning models not only work on the data they were trained on, but
also on unseen (test) data that was not used for training the model. When you use training data
to make any kind of decision (like feature or model selection, hyperparameter tuning, ...),
the data becomes less valuable for generalization to unseen data. So if you just use the public 
leaderboard for testing, you might overfit to the public leaderboard and lose many ranks once the private
leaderboard is revealed.
A better approach is to use validation to get an estimate of performane on unseen data: 

  - First figure out how the Kaggle data was split into train and test data. Your resampling strategy should follow the same method if possible. So if kaggle uses, e.g. a feature for splitting the data, you should not use random samples for creating cross-validation folds.
  - Set up a [*resampling procedure*](https://mlr.mlr-org.com/articles/tutorial/devel/resample.html), e.g., cross-validation (CV) to measure your model performance
  - Improvements on your local CV score should also lead to improvements on the leaderboard. 
  - If this is not the case, you can try
      - several CV folds (e.g., 3-fold, 5-fold, 8-fold)
      - repeated CV (e.g., 3 times 3-fold, 3 times 5-fold)
      - stratified CV
  - `mlr` offers nice [*visualizations to benchmark*](https://mlr.mlr-org.com/articles/tutorial/devel/benchmark_experiments.html#benchmark-analysis-and-visualization) different algorithms.
  
### 6. Ensemble **different** models (see, e.g. [*this guide*](http://mlwave.com/kaggle-ensembling-guide)): 

After training many different models, you might want to ensemble them into one strong model using one of these methods:

  - simple averaging or voting
  - finding optimal weights for averaging or voting
  - stacking
  
### 7. Track your progress

A kaggle project might get quite messy very quickly, because you might try and prototype
many different ideas. To avoid getting lost, make sure to keep track of:

  - What preprocessing steps were used to create the data
  - What model was used for each step
  - What values were predicted in the test file
  - What local score did the model achieve 
  - What public score did the model achieve
  
If you do not want to use a tool like git, at least make sure you create subfolders
for each prototype. This way you can later analyse which models you might want to ensemble
or use for your final commits for the competition.

<!--chapter:end:2017-03-09-Being-successful-on-Kaggle-using-mlr.Rmd-->

---
title: "First release of mlrMBO - the toolbox for (Bayesian) Black-Box Optimization"
authors: ["jakob-richter"]
date: 2017-03-13
categories: ["R", "r-bloggers"]
tags: ["mlrMBO", "Bayesian", "tuning", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

We are happy to finally announce the first release of [**mlrMBO** on cran](https://cran.r-project.org/package=mlrMBO) after a quite long development time.
For the theoretical background and a nearly complete overview of mlrMBOs capabilities you can check our [paper on **mlrMBO** that we presubmitted to arxiv](https://arxiv.org/abs/1703.03373).

The key features of **mlrMBO** are:

* Global optimization of expensive Black-Box functions.
* Multi-Criteria Optimization.
* Parallelization through multi-point proposals.
* Support for optimization over categorical variables using random forests as a surrogate.

For examples covering different scenarios we have Vignettes that are also available as an [online documentation](https://mlrMBO.mlr-org.com/).
For **mlr** users **mlrMBO** is especially interesting for hyperparameter optimization.

<!--more-->

**mlrMBO** for **mlr** hyperparameter tuning was already used in [an earlier blog post](/How-to-win-a-drone-in-20-lines-of-R-code).
Nonetheless we want to provide a small toy example to demonstrate the work flow of **mlrMBO** in this post.

### Example

First, we define an objective function that we are going to minimize:

```{r objectiveFunction}
set.seed(1)
library(mlrMBO)
fun = makeSingleObjectiveFunction(
  name = "SineMixture",
  fn = function(x) sin(x[1])*cos(x[2])/2 + 0.04 * sum(x^2),
  par.set = makeNumericParamSet(id = "x", len = 2, lower = -5, upper = 5)
)
```

To define the objective function we use `makeSingleObjectiveFunction` from the neat package [**smoof**](https://github.com/jakobbossek/smoof), which gives us the benefit amongst others to be able to directly visualize the function.
_If you happen to be in need of functions to optimize and benchmark your optimization algorithm I recommend you to have a look at the package!_

```{r plotObjectiveFunction}
library(plot3D)
plot3D(fun, contour = TRUE, lightning = TRUE)
```

Let's start with the configuration of the optimization:

```{r makeMBOControl}
# In this simple example we construct the control object with the defaults:
ctrl = makeMBOControl()
# For this numeric optimization we are going to use the Expected 
# Improvement as infill criterion:
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
# We will allow for exactly 25 evaluations of the objective function:
ctrl = setMBOControlTermination(ctrl, max.evals = 25L)
```

The optimization has to so start with an initial design.
**mlrMBO** can automatically create one but here we are going to use a randomly sampled LHS design of our own:

```{r design}
library(ggplot2)
des = generateDesign(n = 8L, par.set = getParamSet(fun), 
  fun = lhs::randomLHS)
autoplot(fun, render.levels = TRUE) + geom_point(data = des)
```

The points demonstrate how the initial design already covers the search space but is missing the area of the global minimum.
Before we can start the Bayesian optimization we have to set the surrogate learner to *Kriging*.
Therefore we use an *mlr* regression learner.
In fact, with *mlrMBO* you can use any regression learner integrated in *mlr* as a surrogate allowing for many special optimization applications.

```{r surrogate}
sur.lrn = makeLearner("regr.km", predict.type = "se", 
  config = list(show.learner.output = FALSE))
```

_Note:_ **mlrMBO** can automatically determine a good surrogate learner based on the search space defined for the objective function.
For a purely numeric domain it would have chosen *Kriging* as well with some slight modifications to make it a bit more stable against numerical problems that can occur during optimization.

Finally, we can start the optimization run:

```{r mboRun}
res = mbo(fun = fun, design = des, learner = sur.lrn, control = ctrl, 
  show.info = TRUE)
res$x
res$y
```

We can see that we have found the global optimum of $y = -0.414964$ at $x = (-1.35265,0)$ quite sufficiently.
Let's have a look at the points mlrMBO evaluated.
Therefore we can use the `OptPath` which stores all information about all evaluations during the optimization run:

```{r mboPoints}
opdf = as.data.frame(res$opt.path)
autoplot(fun, render.levels = TRUE, render.contours = FALSE) + 
  geom_text(data = opdf, aes(label = dob))
```

It is interesting to see, that for this run the algorithm first went to the local minimum on the top right in the 6th and 7th iteration but later, thanks to the explorative character of the _Expected Improvement_, found the real global minimum.

### Comparison

That is all good, but how do other optimization strategies perform?

#### Grid Search

Grid search is seldom a good idea.
But especially for hyperparameter tuning it is still used.
Probably because it kind of gives you the feeling that you know what is going on and have not left out any important area of the search space.
In reality the grid is usually so sparse that it leaves important areas untouched as you can see in this example:

```{r gridSeach}
grid.des = generateGridDesign(par.set = getParamSet(fun), resolution = 5)
grid.des$y = apply(grid.des, 1, fun)
grid.des[which.min(grid.des$y),]
autoplot(fun, render.levels = TRUE, render.contours = FALSE) + 
  geom_point(data = grid.des)
```

It is no surprise, that the grid search could not cover the search space well enough and we only reach a bad result.

#### What about a simple random search?

```{r randomSearch}
random.des = generateRandomDesign(par.set = getParamSet(fun), n = 25L)
random.des$y = apply(random.des, 1, fun)
random.des[which.min(random.des$y),]
autoplot(fun, render.levels = TRUE, render.contours = FALSE) + 
  geom_point(data = random.des)
```

With the random search you could always be lucky but in average the optimum is not reached if smarter optimization strategies work well.

#### A fair comarison

... for stochastic optimization algorithms can only be achieved by repeating the runs.
**mlrMBO** is stochastic as the initial design is generated randomly and the fit of the Kriging surrogate is also not deterministic.
Furthermore we should include other optimization strategies like a genetic algorithm and direct competitors like `rBayesOpt`.
An extensive benchmark is available in [our **mlrMBO** paper](https://arxiv.org/abs/1703.03373).
The examples here are just meant to demonstrate the package.

### Engage

If you want to contribute to [**mlrMBO**](https://github.com/mlr-org/mlrMBO) we ware always open to suggestions and pull requests on github.
You are also invited to fork the repository and build and extend your own optimizer based on our toolbox.





<!--chapter:end:2017-03-13-First_release_of_mlrMBO_the_toolbox_for_Bayesian_Black_Box_Optimization.Rmd-->

---
title: "Parallel benchmarking with OpenML and mlr"
authors: ["heidi-seibold"]
date: 2017-03-22
categories: ["R"]
draft: true
tags: ["OpenML", "benchmark", "parallelization", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, eval = FALSE)
```

With this post I want to show you how to benchmark several learners (or learners with different parameter settings) using several data sets in a structured and parallelized fashion.
For this we want to use [`batchtools`](https://mllg.github.io/batchtools/).

The data that we will use here is stored on the open machine learning platform [openml.org](https://www.openml.org/) and we can download it together with information on what to do with it in form of a task.

If you have a small project and don't need to parallelize, you might want to just look at the previous blog post called [mlr loves OpenML](https://mlr-org.com/docs/2016-09-09-mlr-loves-openml/).

The following packages are needed for this:

```{r, eval=TRUE, message=FALSE}
library("OpenML")
library("mlr")
library("batchtools")
library("ggplot2")
```

Now we download five OpenML-tasks from OpenML:

```{r}
set.seed(2017)

## get useful tasks
task_infos = listOMLTasks(tag = "study_14")

## take a sample of 5 tasks from these
task_ids = sample(task_infos$task.id, size = 5)
tasks = lapply(task_ids, getOMLTask)
```

In a next step we need to create the so called registry.
What this basically does is to create a folder with a certain subfolder structure.

```{r, include=FALSE}
## before creating the registry, check if it already exists
## if so, delete it
unlink("parallel_benchmarking_blogpost", recursive = TRUE)
```

```{r}
## create the experiment registry
reg = makeExperimentRegistry(
  file.dir = "parallel_benchmarking_blogpost",
  packages= c("mlr", "OpenML", "party"),
  seed = 123)
names(reg)
reg$cluster.functions

## allow for parallel computing, for other options see ?makeClusterFunctions
# to save ressources, we just use 2 cores here
reg$cluster.functions = makeClusterFunctionsMulticore(2)
```

Now you should have a new folder in your working directory with the name `parallel_benchmarking_blogpost` and the following subfolders / files:

```{}
parallel_benchmarking_blogpost/
├── algorithms
├── exports
├── external
├── jobs
├── logs
├── problems
├── registry.rds
├── results
└── updates
```

In the next step we get to the interesting point.
We need to define...

- the **problems**, which in our case are simply the OpenML tasks we downloaded.
- the **algorithm**, which with mlr and OpenML is quite simply achieved using `makeLearner` and `runTaskMlr`.
We do not have to save the run results (result of applying the learner to the task), but we can directly upload it to OpenML where the results are automatically evaluated.
- the machine learning **experiment**, i.e. in our case which parameters do we want to set for which learner.
As an example here, we will look at the _ctree_ algorithm from the [_party_](https://cran.r-project.org/package=party) package and see whether Bonferroni correction (correction for multiple testing) helps getting better predictions and also we want to check whether we need a tree that has more than two leaf nodes (`stump = FALSE`) or if a small tree is enough (`stump = TRUE`).

```{r}
## add the problem, in our case the tasks from OpenML
for(task in tasks) {
  addProblem(name = paste("omltask", task$task.id, sep = "_"), data = task)
}

##' Function that takes the task (data) and the learner, runs the learner on
##' the task, uploads the run and returns the run ID.
##'
##' @param job required argument for addAlgorithm
##' @param instance required argument for addAlgorithm
##' @param data the task
##' @param learner the string that defines the learner, see listLearners()
runTask_uploadRun = function(job, instance, data, learner, ...) {

  learner = makeLearner(learner, par.vals = list(...))
  run = runTaskMlr(data, learner)

  run_id = uploadOMLRun(run, tag = "test", confirm.upload = FALSE)
  return(run_id)

}

## add the algorithm
addAlgorithm(name = "mlr", fun = runTask_uploadRun)

## what versions of the algorithm do we want to compute
algo.design = list(mlr = expand.grid(
  learner = "classif.ctree",
  testtype = c("Bonferroni", "Univariate"),
  stump = c(FALSE, TRUE),
  stringsAsFactors = FALSE))
algo.design$mlr

addExperiments(algo.designs = algo.design, repls = 1)

## get an overview of what we will submit
summarizeExperiments()
```

Now we can simply run our experiment:

```{r}
submitJobs()
```

While your job is running, you can check the progress using `getStatus()`.
As soon as `getStatus()` tells us that all our runs are done, we can collect the results of our experiment from OpenML.
To be able to do this we need to collect the run IDs from the uploaded runs we did during the experiment.
Also we want to add the info of the parameters used (`getJobPars()`).

```{r}
results0 = reduceResultsDataTable()
job.pars = getJobPars()
results = cbind(run.id = results0$V1, job.pars)
```

With the run ID information we can now grab the evaluations from OpenML and plot for example the parameter settings against the predictive accuracy.

```{r, eval=FALSE, fig.width=8}
run.evals0 = listOMLRunEvaluations(run.id = results$run.id)
run.evals = merge(results, run.evals0, by = "run.id")

ggplot(run.evals, aes(
  x = interaction(testtype, stump),
  y = predictive.accuracy,
  group = data.name,
  color = interaction(task.id, data.name))) +
  geom_point() + geom_line()
```

We see that the only data set where a stump is good enough is the pc1 data set.
For the madelon data set Bonferroni correction helps.
For the others it does not seem to matter.
You can check out the results online by going to the task websites (e.g. for task 9976 for the madelon data set go to [openml.org/t/9976](https://www.openml.org/t/9976)) or the run websites (e.g. [openml.org/r/1852889](https://www.openml.org/r/1852889)).

<!--chapter:end:2017-03-22-Parallel_benchmarking_with_OpenML_and_mlr.Rmd-->

---
title: "Use mlrMBO to optimize via command line"
authors: ["jakob-richter"]
date: 2017-03-22
categories: ["R", "r-bloggers"]
tags: ["mlrMBO", "command-line", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

Many people who want to apply Bayesian optimization want to use it to optimize an algorithm that is not implemented in **R** but runs on the command line as a shell script or an executable.

We recently published [**mlrMBO**](https://mlrMBO.mlr-org.com/) on CRAN.
As a normal package it normally operates inside of R, but with this post I want to demonstrate how **mlrMBO** can be used to optimize an external application.
At the same time I will highlight some issues you can likely run into.

First of all we need a bash script that we want to optimize.
This tutorial will only run on Unix systems (Linux, OSX etc.) but should also be informative for windows users.
The following code will write a tiny bash script that uses `bc` to calculate $sin(x_1-1) + (x_1^2 + x_2^2)$ and write the result "hidden" in a sentence (`The result is 12.34!`) in a `result.txt` text file.

### The bash script

```{r}
# write bash script
lines = '#!/bin/bash
fun ()
{
  x1=$1
  x2=$2
  command="(s($x1-1) + ($x1^2 + $x2^2))"
  result=$(bc -l <<< $command)
}
echo "Start calculation."
fun $1 $2
echo "The result is $result!" > "result.txt"
echo "Finish calculation."
'
writeLines(lines, "fun.sh")
# make it executable:
system("chmod +x fun.sh")
```

### Running the script from R

Now we need a R function that starts the script, reads the result from the text file and returns it.

```{r runScript}
library(stringi)
runScript = function(x) {
  command = sprintf("./fun.sh %f %f", x[['x1']], x[['x2']])
  error.code = system(command)
  if (error.code != 0) {
    stop("Simulation had error.code != 0!")
  }
  result = readLines("result.txt")
  # the pattern matches 12 as well as 12.34 and .34
  # the ?: makes the decimals a non-capturing group.
  result = stri_match_first_regex(result, 
    pattern = "\\d*(?:\\.\\d+)?(?=\\!)")
  as.numeric(result)
}
```

This function uses `stringi` and _regular expressions_ to match the result within the sentence.
Depending on the output different strategies to read the result make sense.
XML files can usually be accessed with `XML::xmlParse`, `XML::getNodeSet`, `XML::xmlAttrs` etc. using `XPath` queries.
Sometimes the good old `read.table()` is also sufficient.
If, for example, the output is written in a file like this:

```{r, eval=FALSE}
value1 = 23.45
value2 = 13.82
```

You can easily use `source()` like that:

```{r, eval=FALSE}
EV = new.env()
eval(expr = {a = 1}, envir = EV)
as.list(EV)
source(file = "result.txt", local = EV)
res = as.list(EV)
rm(EV)
```

which will return a list with the entries `$value1` and `$value2`.

### Define bounds, wrap function.

To evaluate the function from within **mlrMBO** it has to be wrapped in **smoof** function.
The smoof function also contains information about the bounds and scales of the domain of the objective function defined in a _ParameterSet_.

```{r smoof}
library(mlrMBO)
# Defining the bounds of the parameters:
par.set = makeParamSet(
  makeNumericParam("x1", lower = -3, upper = 3),
  makeNumericParam("x2", lower = -2.5, upper = 2.5)
)
# Wrapping everything in a smoof function:
fn = makeSingleObjectiveFunction(
  id = "fun.sh", 
  fn = runScript,
  par.set = par.set,
  has.simple.signature = FALSE
)

# let's see if the function is working
des = generateGridDesign(par.set, resolution = 3)
des$y = apply(des, 1, fn)
des
```

If you run this locally, you will see that the console output generated by our shell script directly appears in the R-console.
This can be helpful but also annoying.

### Redirecting output

If a lot of output is generated during a single call of `system()` it might even crash R.
To avoid that I suggest to redirect the output into a file.
This way no output is lost and the R console does not get flooded.
We can simply achieve that by replacing the `command` in the function `runScript` from above with the following code:

```{r redirectOutput, eval = FALSE}
  # console output file output_1490030005_1.1_2.4.txt
  output_file = sprintf("output_%i_%.1f_%.1f.txt", 
    as.integer(Sys.time()), x[['x1']], x[['x2']])
  # redirect output with ./fun.sh 1.1 2.4 > output.txt
  # alternative: ./fun.sh 1.1 2.4 > /dev/null to drop it
  command = sprintf("./fun.sh %f %f > %s", x[['x1']], x[['x2']], output_file)
```

```{r smoof_redirectOutput, include=FALSE}
fn = makeSingleObjectiveFunction(
  id = "fun.sh", 
  fn = runScript,
  par.set = par.set,
  has.simple.signature = FALSE
)
```
 
### Start the Optimization

Now everything is set so we can proceed with the usual MBO setup:

```{r runMBO}
ctrl = makeMBOControl()
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
ctrl = setMBOControlTermination(ctrl, iters = 10)
configureMlr(show.info = FALSE, show.learner.output = FALSE)
run = mbo(fun = fn, control = ctrl)
# The resulting optimal configuration:
run$x
# The best reached value:
run$y
```

### Execute the R script from a shell

Also you might not want to bothered having to start *R* and run this script manually so what I would recommend is saving all above as an R-script plus some lines that write the output in a JSON file like this:

```{r saveRes}
library(jsonlite)
write_json(run[c("x","y")], "mbo_res.json")
```

Let's assume we saved all of that above as an R-script under the name [`runMBO.R` (actually it is available as a gist)](https://gist.github.com/jakob-r/6be022d49e135c7905fd4c097bc3d376).

Then you can simply run it from the command line:

```{bash, eval = FALSE}
Rscript runMBO.R 
```

As an extra the script in the gist also contains a simple handler for command line arguments.
In this case you can define the number of optimization iterations and the maximal allowed time in seconds for the optimization.
You can also define the seed to make runs reproducible:

```{bash, eval = FALSE}
Rscript runMBO.R iters=20 time=10 seed=3
```

If you want to build a more advanced command line interface you might want to have a [look](https://www.slideshare.net/EdwindeJonge1/docopt-user2014) [at](https://github.com/docopt/docopt.R) [docopt](https://cran.r-project.org/package=docopt).

### Clean up

To clean up all the files generated by this script you can run:
```{r cleanUp, results='hide'}
file.remove("result.txt")
file.remove("fun.sh")
file.remove("mbo_res.json")
output.files = list.files(pattern = "output_\\d+_[0-9_.-]+\\.txt")
file.remove(output.files)
```

<!--chapter:end:2017-03-22-Use_mlrMBO_to_optimize_via_command_line.Rmd-->

---
title: "New mlr Logo"
authors: ["mason-gallo"]
date: 2017-03-24
categories: ["R", "r-bloggers"]
tags: ["mlr", "changelog", "logo"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

We at mlr are currently deciding on a new logo, and in the spirit of open-source, we would like to involve the community in the voting process!

You can vote for your favorite logo on [GitHub](https://github.com/mlr-org/mlr/issues/1684) by reacting to the logo with a +1.

Thanks to [Hannah Atkin](https://dribbble.com/hratkin) for designing the logos!


<!--chapter:end:2017-03-23-New-mlr-Logo.Rmd-->

---
title: "Multilabel Classification with mlr"
authors: ["quay-au"]
date: 2017-03-28
categories: ["R"]
tags: ["classification", "multilabel", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

Multilabel classification has lately gained growing interest in the research community.
We implemented several methods, which make use of the standardized mlr framework. Every available binary learner can be used for multilabel problem transformation methods.
So if you're interested in using several multilabel algorithms and want to know how to use them in the mlr framework, then this post is for you!

### 1) Introduction to multilabel classification

First, let me introduce you to multilabel classification. This is a classification problem, where every instance can have more than one label. Let's have a look at a typical multilabel dataset (which I, of course, download from the [OpenML server](https://www.openml.org/search?q=2016_multilabel_r_benchmark_paper)):

```{r, results='hide', message=FALSE, warning=FALSE}
library(mlr)
library(OpenML)
setOMLConfig(apikey = "c1994bdb7ecb3c6f3c8f3b35f4b47f1f") # api key
oml.id = listOMLDataSets(tag = "2016_multilabel_r_benchmark_paper")$data.id
scene = getOMLDataSet(data.id = oml.id[8])
target = scene$target.features
feats = setdiff(colnames(scene$data), target)
```

```{r}
head(scene$data[, c(feats[1], feats[2], target)])
```

Here I took the [*scene*](http://www.sciencedirect.com/science/article/pii/S0031320304001074) dataset, where the features represent color information of pictures and the targets could be objects like *beach*, *sunset*, and so on.

As you can see above, one defining property of a multilabel dataset is, that the target variables (which are called *labels*) are binary. If you want to use your own data set, make sure to encode these variables in *logical*, where *TRUE* indicates the relevance of a label.

The basic idea behind many multilabel classification algorithms is to make use of possible correlation between labels. Maybe a learner is very good at predicting label 1, but rather bad at predicting label 2. If label 1 and label 2 are highly correlated, it may be beneficial to predict label 1 first and use this prediction as a feature for predicting label 2.

This approach is the main concept behind the so called *problem transformation methods*. The multilabel problem is transformed into binary classification problems, one for each label. Predicted labels are used as features for predicting other labels.

We implemented the following problem transformation methods:

* Classifier chains
* Nested stacking
* Dependent binary relevance
* Stacking

How these methods are defined, can be read in the [mlr tutorial](https://mlr.mlr-org.com/articles/tutorial/multilabel.html) or in more detail in our [paper](https://arxiv.org/pdf/1703.08991.pdf). Enough theory now, let's apply these methods on our dataset.

### 2) Let's Train and Predict!

First we need to create a multilabel task.
```{r}
set.seed(1729)
target
scene.task = makeMultilabelTask(data = scene$data, target = target)
```

We set a seed, because the classifier chain wrapper uses a random chain order.
Next, we train a learner. I chose the classifier chain approach together with a decision tree for the binary classification problems.

```{r}
binary.learner = makeLearner("classif.rpart")
lrncc = makeMultilabelClassifierChainsWrapper(binary.learner)
```

Now let's train and predict on our dataset:

```{r}
n = getTaskSize(scene.task)
train.set = seq(1, n, by = 2)
test.set = seq(2, n, by = 2)

scene.mod.cc = train(lrncc, scene.task, subset = train.set)
scene.pred.cc = predict(scene.mod.cc, task = scene.task, subset = test.set)
```

We also implemented common multilabel performance measures. Here is a list with [available multilabel performance measures](https://mlr.mlr-org.com/articles/tutorial/measures.html#multilabel-classification):
```{r}
listMeasures("multilabel")
```

Here is how the classifier chains method performed:
```{r}
performance(scene.pred.cc, measures = list(multilabel.hamloss, 
  multilabel.subset01, multilabel.f1, multilabel.acc))
```

### 3) Comparison Binary Relevance vs. Classifier Chains

Now let's see if it can be beneficial to use predicted labels as features for other labels. Let us compare the performance of the classifier chains method with the binary relevance method (this method does not use predicted labels as features).

```{r}
lrnbr = makeMultilabelBinaryRelevanceWrapper(binary.learner)

scene.mod.br = train(lrnbr, scene.task, subset = train.set)
scene.pred.br = predict(scene.mod.br, task = scene.task, subset = test.set)

performance(scene.pred.br, measures = list(multilabel.hamloss, 
  multilabel.subset01, multilabel.f1, multilabel.acc))
```

As can be seen here, it could indeed make sense to use more elaborate methods for multilabel classification, since classifier chains beat the binary relevance methods in all of these measures (Note, that hamming loss and subset01 are loss measures!).

### 4) Resampling

Here I'll show you how to use resampling methods in the multilabel setting. Resampling methods are key for assessing the performance of a learning algorithm. To read more about resampling, see the page on our [tutorial](https://mlr.mlr-org.com/articles/tutorial/devel/resample.html).

First, we need to define a resampling strategy. I chose subsampling, which is also called Monte-Carlo cross-validation. The dataset is split into training and test set at a predefined ratio. The learner is trained on the training set, the performance is evaluated with the test set. This whole process is repeated many times and the performance values are averaged. In mlr this is done the following way:

```{r}
rdesc = makeResampleDesc("Subsample", iters = 10, split = 2/3)
```

Now we can choose a measure, which shall be resampled. All there is left to do is to run the resampling:

```{r, message = FALSE}
r = resample(lrncc, scene.task, rdesc, measures = multilabel.subset01)
```

```{r}
r
```

If you followed the mlr tutorial or if you are already familiar with mlr, you most likely saw, that using resampling in the multilabel setting isn't any different than generally using resampling in mlr.
Many methods, which are available in mlr, like [preprocessing](https://mlr.mlr-org.com/articles/tutorial/preproc.html), [tuning](https://mlr.mlr-org.com/articles/tutorial/tune.html) or [benchmark experiments](https://mlr.mlr-org.com/articles/tutorial/benchmark_experiments.html) can also be used for multilabel datasets and the good thing here is: the syntax stays the same!

<!--chapter:end:2017-03-28-Multilabel-Classification-with-mlr.Rmd-->

---
title: "Most Popular Learners in mlr"
authors: ["jakob-richter"]
date: 2017-03-30
categories: ["R", "r-bloggers"]
tags: ["learner", "algorithm", "popular", "machine-learning", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(mlr)
library(stringi)
library(cranlogs)
library(data.table)
library(ggplot2)
```

For the development of [mlr](https://github.com/mlr-org/mlr) as well as for an "machine learning expert" it can be handy to know what are the most popular learners used.
Not necessarily to see, what are the top notch performing methods but to see what is used "out there" in the real world. 
Thanks to the nice little package [cranlogs](https://github.com/metacran/cranlogs) from [metacran](https://www.r-pkg.org/) you can at least get a slight estimate as I will show in the following...

First we need to install the `cranlogs` package using `devtools`:

```{r install cranlogs, eval=FALSE}
devtools::install_github("metacran/cranlogs")
```

Now let's load all the packages we will need:

```{r libraries}
library(mlr)
library(stringi)
library(cranlogs)
library(data.table)
```

Do obtain a neat table of all available learners in _mlr_ we can call `listLearners()`.
This table also contains a column with the needed packages for each learner separated with a `,`.

```{r listLearners, warning=FALSE}
# obtain used packages for all learners
lrns = as.data.table(listLearners())
all.pkgs = stri_split(lrns$package, fixed = ",")
```

_Note:_ You might get some warnings here because you likely did not install all packages that _mlr_ suggests -- which is totally fine.

Now we can obtain the download counts from the _rstudio cran mirror_, i.e. from the last month.
We use `data.table` to easily sum up the download counts of each day.

```{r downloadCounts}
all.downloads = cran_downloads(packages = unique(unlist(all.pkgs)), 
                               when = "last-month")
all.downloads = as.data.table(all.downloads)
monthly.downloads = all.downloads[, list(monthly = sum(count)), by = package]
```

As some learners need multiple packages we will use the download count of the package with the least downloads.

```{r aggregateDownloads}
lrn.downloads = sapply(all.pkgs, function(pkgs) {
  monthly.downloads[package %in% pkgs, min(monthly)]
})
```

Let's put these numbers in our table:

```{r firstTable, results='hide'}
lrns$downloads = lrn.downloads
lrns = lrns[order(downloads, decreasing = TRUE),]
lrns[, .(class, name, package, downloads)]
```

_Here are the first 5 rows of the table:_

```{r firstTableKnit, echo=FALSE, results='asis'}
knitr::kable(lrns[, .(class, name, package, downloads)][1:5])
```

Now let's get rid of the duplicates introduced by the distinction of the type _classif_, _regr_ and we already have our...

## Nearly final table

```{r finalTable, results='hide'}
lrns.small = lrns[, .SD[1,], by = .(name, package)]
lrns.small[, .(class, name, package, downloads)]
```

The top 20 according to the _rstudio cran mirror_:

```{r finalTableKnit, echo=FALSE, results='asis'}
knitr::kable(lrns.small[, .(class, name, package, downloads)][1:20])
```

As we are just looking for the packages let's compress the table a bit further and come to our...

## Final table

```{r compressTable, results='hide'}
lrns.pgks = lrns[,list(learners = paste(class, collapse = ",")),
                 by = .(package, downloads)]
lrns.pgks
```

_Here are the first 20 rows of the table:_

```{r compressTableKnit, echo=FALSE, results='asis'}
knitr::kable(lrns.pgks[1:20,])
```

And of course we want to have a small visualization:

```{r compressTablePlot}
library(ggplot2)
library(forcats)
lrns.pgks$learners = factor(lrns.pgks$learners, lrns.pgks$learners)
g = ggplot(lrns.pgks[20:1], aes(x = fct_inorder(stri_sub(
  paste0(package,": ",learners), 0, 64)), y = downloads, fill = downloads))
g + geom_bar(stat = "identity") + 
  coord_flip() + 
  xlab("") + 
  scale_fill_continuous(guide=FALSE)
```

## Remarks

This is not really representative of how popular each learner is, as some packages have multiple purposes (e.g. multiple learners).
Furthermore it would be great to have access to the [trending](https://www.r-pkg.org/trending) list.
Also [_most stars at GitHub_](https://www.r-pkg.org/starred) gives a better view of what the developers are interested in.
Looking for machine learning packages we see there e.g: [xgboost](https://github.com/dmlc/xgboost), [h2o](https://github.com/h2oai/h2o-3) and [tensorflow](https://github.com/rstudio/tensorflow).

<!--chapter:end:2017-03-30-Most_Popular_Learners_in_mlr.Rmd-->

---
title: "shinyMlr"
authors: ["florian-fendt"]
date: 2017-05-16
categories: ["R", "r-bloggers"]
tags: ["shiny", "mlr", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

shinyMlr is a web application, built with the
[R-package "shiny"](https://cran.r-project.org/web/packages/shiny/index.html)
that provides a user interface for [mlr](https://github.com/mlr-org/mlr).
By wrapping the main functionalities of mlr into our app, as well as
implementing additional features for data visualisation and data preprocessing,
we built a widely usable application for your day to day
machine learning tasks, which we would like to present to you today.

[Stefan](http://Coorsaa.github.io/) and [me](http://florianfendt.github.io/)
started working on this project late summer 2016 as part of a practical
course we attended for our Master's program. We enjoyed the work on this
project and will continue to maintain and extend our app in the future.
However, after almost one year of work our application got a versatile tool
and it is time to present it to a broader audience.
To introduce you to the workflow and main features of our app,
we uploaded a video series to our
[youtube channel](https://www.youtube.com/channel/UCxafH0u7ewWHEULXcEa_zkg).
The videos are little tutorials that illustrate the workflow
in form of a use case:
We used the titanic data set from the
[kaggle competition](https://www.kaggle.com/c/titanic)
as example data to show you step by step how it can be analyzed with our
application.

The first video gives a small introduction and shows you how data can be
imported:

```{r echo=FALSE}
blogdown::shortcode('youtube', 'VZuRSQZdayY')
```

In the next tutorial you will learn how to visualise your data and
preprocess it:

```{r echo=FALSE}
blogdown::shortcode('youtube', 'kKV4mg-wRts')
```

The third and fourth screencasts show you how to create your task and
how to construct and modify our built-in learning algorithms:

```{r echo=FALSE}
blogdown::shortcode('youtube', 'HPr-9dBbv6o')
```

```{r echo=FALSE}
blogdown::shortcode('youtube', 'SjeQzvEjoaQ')
```

The fifth part of our tutorials shows you how to tune your learners to find
suitable parameter settings for your given training set:

```{r echo=FALSE}
blogdown::shortcode('youtube', 'gMh4ZhRQhJI')
```

The sixth video gives you detailed information on how to actually train models
on your task, predict on new data and plot model diagnostic and prediction
plots:

```{r echo=FALSE}
blogdown::shortcode('youtube', 'Fxbc5YmI_i8')
```

The seventh video runs a benchmark experiment, to show you how to compare
different learners in our application:

```{r echo=FALSE}
blogdown::shortcode('youtube', '3Y1cfxtUCNY')
```

The last tutorial briefly demonstrates how to render an interactive report
from your analysis done with our app:

```{r echo=FALSE}
blogdown::shortcode('youtube', 'UxHHYem4-s8')
```

I hope you enjoyed watching the videos and learned how to make use of our
application.
If you like working with our app please leave us a star and follow us on
[github](https://github.com/mlr-org/shinyMlr)

<!--chapter:end:2017-05-16-shinyMlr.Rmd-->

---
title: "Parameter tuning with mlrHyperopt"
authors: ["jakob-richter"]
date: 2017-07-19
categories: ["R", "r-bloggers"]
tags: ["tuning", "hyperparameter", "optimization", "mlrHyperopt", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
set.seed(123)
library(mlr)
mlr::configureMlr(show.info = FALSE, show.learner.output = FALSE)
```

Hyperparameter tuning with [**mlr**](https://github.com/mlr-org/mlr#-machine-learning-in-r) is rich in options as they are multiple tuning methods:

* Simple Random Search
* Grid Search
* Iterated F-Racing (via [**irace**](http://iridia.ulb.ac.be/irace/))
* Sequential Model-Based Optimization (via [**mlrMBO**](https://mlrMBO.mlr-org.com/))

Also the search space is easily definable and customizable for each of the [60+ learners of mlr](https://mlr.mlr-org.com/articles/tutorial/devel/integrated_learners.html) using the ParamSets from the [**ParamHelpers**](https://github.com/berndbischl/ParamHelpers) Package.

The only drawback and shortcoming of **mlr** in comparison to [**caret**](http://topepo.github.io/caret/index.html) in this regard is that **mlr** itself does not have defaults for the search spaces.
This is where [**mlrHyperopt**](http://jakob-r.de/mlrHyperopt/) comes into play.

**mlrHyperopt** offers

* default search spaces for the most important learners in **mlr**,
* parameter tuning in one line of code,
* and an API to add and access custom search spaces from the [mlrHyperopt Database](http://mlrhyperopt.jakob-r.de/parconfigs).

### Installation

```{r installation, eval = FALSE}
# version >= 1.11 needed.
devtools::install_github("berndbischl/ParamHelpers") 
devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
```

### Tuning in one line

Tuning can be done in one line relying on the defaults.
The default will automatically minimize the _missclassification rate_.

```{r objectiveFunction, warning=FALSE, message=FALSE}
library(mlrHyperopt)
res = hyperopt(iris.task, learner = "classif.svm")
res
```

We can find out what `hyperopt` did by inspecting the `res` object.

Depending on the parameter space **mlrHyperopt** will automatically decide for a suitable tuning method:

```{r resObjectControl}
res$opt.path$par.set
res$control
```

As the search space defined in the ParamSet is only numeric, sequential Bayesian optimization was chosen.
We can look into the evaluated parameter configurations and we can visualize the optimization run.

```{r resObjectOptPath, message=FALSE}
tail(as.data.frame(res$opt.path))
plotOptPath(res$opt.path)
```

The upper left plot shows the distribution of the tried settings in the search space and contour lines indicate where regions of good configurations are located.
The lower right plot shows the value of the objective (the miss-classification rate) and how it decreases over the time. 
This also shows nicely that wrong settings can lead to bad results.

### Using the mlrHyperopt API with mlr

If you just want to use **mlrHyperopt** to access the default parameter search spaces from the 
Often you don't want to rely on the default procedures of **mlrHyperopt** and just incorporate it into your **mlr**-workflow.
Here is one example how you can use the default search spaces for an easy benchmark:

```{r seed4benchmark, include=FALSE}
set.seed(3)
```

```{r benchmark, message=FALSE, warning=FALSE}
lrns = c("classif.xgboost", "classif.nnet")
lrns = makeLearners(lrns)
tsk = pid.task
rr = makeResampleDesc('CV', stratify = TRUE, iters = 10)
lrns.tuned = lapply(lrns, function(lrn) {
  if (getLearnerName(lrn) == "xgboost") {
    # for xgboost we download a custom ParConfig from the Database
    pcs = downloadParConfigs(learner.name = getLearnerName(lrn))
    pc = pcs[[1]]
  } else {
    pc = getDefaultParConfig(learner = lrn)
  }
  ps = getParConfigParSet(pc)
  # some parameters are dependend on the data (eg. the number of columns)
  ps = evaluateParamExpressions(ps, 
    dict = mlrHyperopt::getTaskDictionary(task = tsk))
  lrn = setHyperPars(lrn, par.vals = getParConfigParVals(pc))
  ctrl = makeTuneControlRandom(maxit = 20)
  makeTuneWrapper(learner = lrn, resampling = rr, par.set = ps, 
                  control = ctrl)
})
res = benchmark(learners = c(lrns, lrns.tuned), tasks = tsk, 
                resamplings = cv10)
plotBMRBoxplots(res) 
```

As we can see we were able to improve the performance of xgboost and the nnet without any additional knowledge on what parameters we should tune.
Especially for nnet improved performance is noticable.

### Additional Information

Some recommended additional reads

* [Vignette](http://jakob-r.de/mlrHyperopt/articles/mlrHyperopt.html) on getting started and also how to contribute by uploading alternative or additional ParConfigs.
* [How to work with ParamSets](http://jakob-r.de/mlrHyperopt/articles/working_with_parconfigs_and_paramsets.html#the-basics-of-a-paramset) as part of the [Vignette](http://jakob-r.de/mlrHyperopt/articles/working_with_parconfigs_and_paramsets.html).
* The [slides of the useR 2017 Talk](https://github.com/jakob-r/mlrHyperopt/raw/master/meta/useR2017/beamer/jakob_richter_mlrHyperopt.pdf) on **mlrHyperopt**.

<!--chapter:end:2017-07-19-Parameter-tuning-with-mlrHyperopt.Rmd-->

---
title: "OpenML Workshop 2017"
authors: ["giuseppe-casalicchio"]
date: 2017-09-01
categories: ["R"]
tags: ["workshop", "tutorial", "mlr", "rstats"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

## What is OpenML?

The field of Machine Learning has grown tremendously over the last years, and is a key component of data-driven science. Data analysis algorithms are being invented and used every day, but their results and experiments are published almost exclusively in journals or separated repositories. However, data by itself has no value. It’s the ever-changing ecosystem surrounding data that gives it meaning.

OpenML is a networked science platform that aims to connect and organize all this knowledge online, linking data, algorithms, results and people into a coherent whole so that scientists and practitioners can easy build on prior work and collaborate in real time online.

OpenML has an online interface on openml.org, and is integrated in the most popular machine learning tools and statistical environments such as R, Python, WEKA, MOA and RapidMiner. This allows researchers and students to easily import and export data from these tools and share them with others online, fully integrated into the context of the state of the art. On OpenML, researchers can connect to each other, start projects, and build on the results of others. It automatically keeps track of how often shared work is reused so that researchers can follow the wider impact of their work and become more visible.

## When and Where?

The OpenML workshop is organized as a hackathon, an event where participants from many scientific domains present their goals and ideas, and then work on them in small teams for many hours or days at a time. Participants bring their laptops, learn how to use OpenML in tutorials, and build upon that to create something great to push their research forward. The complete OpenML development team will be available to get them started, answer questions, and implement new features on the fly.

The next OpenML Workshop will run from 9 October to 13 Oktober 2017, see also [here](http://hackathon.openml.org). 

<!--chapter:end:2017-09-01-OpenML-workshop-2017.Rmd-->

---
title: "Team Rtus wins Munich Re Datathon with mlr"
authors: ["jann-goschenhofer"]
date: 2017-12-14
categories: ["R"]
tags: ["mlr", "hackathon"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

On the weekend of November 17. - 19. five brave data-knights from team “Rtus and the knights of the data.table” took on the challenge to compete in a datathon organized by Munich Re in its Munich-based innovation lab. Team Rtus was formed in April this year by a bunch of statistics students from LMU with the purpose to prove their data-skills in competitions with other teams from various backgrounds. The datathon was centered around the topic “the effects of climate change and hurricane events on modern reinsurance business” and after two days of very intensive battles with databases, web-crawlers and advanced machine learning modelling (using the famous R-library [mlr](https://mlr.mlr-org.com/)), they managed to prevail against strong professional competitors and won the best overall price. After victories at the [Datafest at Mannheim University](https://hiwissml.github.io/datafest2017.github.io/) and the [Telefonica data challenge](https://www.tefdatachallenge.com/) earlier this year, this was the last step to a hattrick for the core members of team Rtus.

Their way to success was the implementation of an interactive web-app that could serve as a decision support system for underwriting and tariffing units at Munich Re that are dealing with natural catastrophies. Munich Re provided the participants with databases on claims exposure in the Florida-bay, footprints of past hurricanes and tons of data on climate variable measurements over the past decades. One of the core tasks of the challenge was to define and calculate the maximum foreseeable loss and the probability of such a worst-case event to take place.

To answer the first question, they created a web app that calculates the expected loss of a hurricane in a certain region. To give the decision makers the opportunity to include their expert domain knowledge, they could interact with the app and set the shape and the location of the hurricane, which was modelled as a spatial Gaussian process. This is depicted in the first screenshot. Due to a NDA the true figures and descriptions in the app were altered.

![Super Gauss App in normal mode.](/images/2017-12-14-Team-Rtus-wins-MunichRe-Datathon/max_nonpp.png)

The team recognised the existence of several critical nuclear power plants in this area. The shocking event of Fukushima in 2011 showed the disastrous effects that storm surges, a side effect of hurricanes, can have in combination with nuclear power plants. To account for this, team Rtus implemented the “Nuclear Power Plant” mode in the app. The function of this NPP-mode is shown in this figure:

![Super Gauss App in Nuclear Power Plant mode.](/images/2017-12-14-Team-Rtus-wins-MunichRe-Datathon/max_npp.png)

In a next step, the team tried to provide evidence for the plausibility of such a worst-case event. The following image, based on the footprints of past hurricanes, shows that there were indeed hurricanes crossing the locations of the nuclear power plants:

![Storm plot to proove plausibility of the worst case.](/images/2017-12-14-Team-Rtus-wins-MunichRe-Datathon/storm.png)

To answer the second part of the question, they also created a simulation of several weather variables to forecast the probability of such heavy category 5 hurricane events. One rule of reliable statistic modelling is the inclusion of uncertainty measures in any prediction, which was integrated via the prediction intervals. Also, the user of the simulation is able to increase or decrease the estimated temperature trend that underlies the model. This screenshot illustrates the simulation app:

![Simulation of probability of heavy hurricanes to occur.](/images/2017-12-14-Team-Rtus-wins-MunichRe-Datathon/sim.png)

The 36 hours of intensive hacking, discussions, reiterations and fantastic team work combined with the consumption of estimated 19.68 litres of Club Mate were finally rewarded with the first place ranking and a Microsoft Surface Pro for each of the knights. “We will apply this augmentation of our weapon arsenal directly in the next data battle”, one of the knights proudly stated during the awards ceremony.

This portrait shows the tired but happy data knights (from left to right: Niklas Klein, Moritz Herrmann, Jann Goschenhofer, Markus Dumke and Daniel Schalk):

![The succesfull team from left to right: Niklas Klein, Moritz Herrmann, Jann Goschenhofer, Markus Dumke and Daniel Schalk.](/images/2017-12-14-Team-Rtus-wins-MunichRe-Datathon/knights.jpg)


<!--chapter:end:2017-12-14-Team-Rtus-wins-MunichRe-Datathon.Rmd-->

---
title: "Stepwise Bayesian Optimization with mlrMBO"
authors: ["jakob-richter"]
date: 2018-01-10
categories: ["R", "r-bloggers"]
tags: ["mlrMBO", "Bayesian", "optimization", "tuning", "stepwise"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(mlr)
library(mlrMBO)
library(ggplot2)
library(rgenoud)
```
  
With the release of the new version of [mlrMBO](https://mlrMBO.mlr-org.com/) we added some minor fixes and added a practical feature called *[Human-in-the-loop MBO](https://mlrMBO.mlr-org.com/articles/supplementary/human_in_the_loop_MBO.html)*.
It enables you to sequentially

* visualize the state of the surrogate model,
* obtain the suggested parameter configuration for the next iteration and
* update the surrogate model with arbitrary evaluations.

In the following we will demonstrate this feature on a simple example.

First we need an objective function we want to optimize.
For this post a simple function will suffice but note that this function could also be an external process as in this mode **mlrMBO** does not need to access the objective function as you will only have to pass the results of the function to **mlrMBO**.

```{r function}
library(mlrMBO)
library(ggplot2)
set.seed(1)

fun = function(x) {
  x^2 + sin(2 * pi * x) * cos(0.3 * pi * x)
}
```

However we still need to define the our search space.
In this case we look for a real valued value between -3 and 3.
For more hints about how to define ParamSets you can look [here](http://jakob-r.de/mlrHyperopt/articles/working_with_parconfigs_and_paramsets.html#the-basics-of-a-paramset) or in the [help of ParamHelpers](https://rdrr.io/cran/ParamHelpers/man/makeParamSet.html).

```{r parmset}
ps = makeParamSet(
  makeNumericParam("x", lower = -3, upper = 3)
)
```

We also need some initial evaluations to start the optimization.
The design has to be passed as a `data.frame` with one column for each dimension of the search space and one column `y` for the outcomes of the objective function.

```{r design}
des = generateDesign(n = 3, par.set = ps)
des$y = apply(des, 1, fun)
des
```

With these values we can initialize our sequential MBO object.

```{r control}
ctrl = makeMBOControl()
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
opt.state = initSMBO(
  par.set = ps, 
  design = des, 
  control = ctrl, 
  minimize = TRUE, 
  noisy = FALSE)
```

The `opt.state` now contains all necessary information for the optimization.
We can even plot it to see how the Gaussian process models the objective function.

```{r optstate1}
plot(opt.state)
```

In the first panel the *expected improvement* ($EI = E(y_{min}-\hat{y})$) (see [Jones et.al.](http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/$FILE/Jones98.pdf)) is plotted over the search space.
The maximum of the *EI* indicates the point that we should evaluate next.
The second panel shows the mean prediction of the surrogate model, which is the Gaussian regression model aka *Kriging* in this example.
The third panel shows the uncertainty prediction of the surrogate.
We can see, that the *EI* is high at points, where the mean prediction is low and/or the uncertainty is high.

To obtain the specific configuration suggested by mlrMBO for the next evaluation of the objective we can run:

```{r suggest1}
prop = proposePoints(opt.state)
prop
```

We will execute our objective function with the suggested value for `x` and feed it back to mlrMBO:

```{r eval1}
y = fun(prop$prop.points$x)
y
updateSMBO(opt.state, x = prop$prop.points, y = y)
```

The nice thing about the *human-in-the-loop* mode is, that you don't have to stick to the suggestion.
In other words we can feed the model with values without receiving a proposal.
Let's assume we have an expert who tells us to evaluate the values $x=-1$ and $x=1$ we can easily do so:

```{r feedmanual}
custom.prop = data.frame(x = c(-1,1))
ys = apply(custom.prop, 1, fun)
updateSMBO(opt.state, x = custom.prop, y = as.list(ys))
plot(opt.state, scale.panels = TRUE)
```

We can also automate the process easily:

```{r evalloop, results='hide'}
replicate(3, {
  prop = proposePoints(opt.state)
  y = fun(prop$prop.points$x)
  updateSMBO(opt.state, x = prop$prop.points, y = y)
})
```

*Note:* We suggest to use the normal mlrMBO if you are only doing this as mlrMBO has more advanced logging, termination and handling of errors etc.

Let's see how the surrogate models the true objective function after having seen seven configurations:

```{r optstate2}
plot(opt.state, scale.panels = TRUE)
```

You can convert the `opt.state` object from this run to a normal mlrMBO result object like this:

```{r finalize}
res = finalizeSMBO(opt.state)
res
```

*Note:* You can always run the *human-in-the-loop MBO* on `res$final.opt.state`.

For the curious, let's see how our original function actually looks like and which points we evaluated during our optimization:

```{r plottrue}
plot(fun, -3, 3)
points(x = getOptPathX(res$opt.path)$x, y = getOptPathY(res$opt.path))
```

We can see, that we got pretty close to the global optimum and that the surrogate in the previous plot models the objective quite accurate.

For more in-depth information look at the [Vignette for Human-in-the-loop MBO](https://mlrMBO.mlr-org.com/articles/supplementary/human_in_the_loop_MBO.html) and check out the other topics of our [mlrMBO page](https://mlrMBO.mlr-org.com/).

<!--chapter:end:2018-01-10-Stepwise-Bayesian-Optimization-with-mlrMBO.Rmd-->

---
title: "Training Courses for mlr: Machine Learning in R"
authors: ["giuseppe-casalicchio"]
date: 2018-02-28
categories: ["R", "r-bloggers"]
tags: ["tutorial", "workshop", "training", "mlr"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

The [`mlr`: Machine Learning in R](http://www.jmlr.org/papers/v17/15-066.html) package provides a generic, object-oriented and extensible framework for classification, regression, survival analysis and clustering for the statistical programming language R. 
The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment.

We are happy to announce that we now offer training courses specialized on `mlr`:

- The [Munich R Courses](http://www.muenchner-r-kurse.de/einfuehrung-maschinelles-lernen/) already offer the three-day course "Machine Learning and Data Mining in R". <!--more-->
The course includes a basic introduction to theoretical concepts of machine learning and especially focuses on the [`mlr` package](https://github.com/mlr-org/mlr).
The next course starts on May 2nd, 2018 and will be held in German (see [here](http://www.muenchner-r-kurse.de/kurs/machine-learning/)).
- The [Professional Certificate Program "Data Science"](http://www.datascience-certificate.de) also includes a short introduction to the `mlr` package. It is a brand new extra-occupational 10-day training at the University of Munich (LMU). 
The certificate program starts in May 2018 and the application deadline is March 15th, 2018. 
- You can request an inhouse R course in English or German via our [contact form.](http://www.muenchner-r-kurse.de/inhouse-schulungen/)
- If you offer R or Data Science courses and want to include a course on [`mlr`: Machine Learning in R](http://www.jmlr.org/papers/v17/15-066.html) into your course program, feel free to contact us at [rkurse@stat.uni-muenchen.de](rkurse@stat.uni-muenchen.de).

<div style="text-align:center"><img src ="../images/mlrLogo_blue_on_white320square.png" /></div>

<!--chapter:end:2018-02-28-mlr-training-courses.Rmd-->

---
title: "Interpretable Machine Learning with iml and mlr"
authors: ["christoph-molnar"]
date: 2018-02-28
categories: ["R", "r-bloggers"]
tags: ["iml", "interpretable", "machine-learning", "mlr", "rstats"]
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", warning = FALSE)
options(tibble.print_min = 4L, tibble.print_max = 4L)
set.seed(1014)
```

Machine learning models repeatedly outperform interpretable, parametric models like the linear regression model. 
The gains in performance have a price: The models operate as black boxes which are not interpretable.

Fortunately, there are many methods that can make machine learning models interpretable. 
The R package `iml` provides tools for analysing any black box machine learning model:

* Feature importance: Which were the most important features?
* Feature effects: How does a feature influence the prediction? (Partial dependence plots and individual conditional expectation curves)
* Explanations for single predictions: How did the feature values of a single data point affect its prediction?  (LIME and Shapley value)
* Surrogate trees: Can we approximate the underlying black box model with a short decision tree?
* The iml package works for any classification and regression machine learning model: random forests, linear models, neural networks, xgboost, etc.

This blog post shows you how to use the `iml` package to analyse machine learning models. 
While the `mlr` package makes it super easy to train machine learning models, the `iml` package makes it easy to extract insights about the learned black box machine learning models.

If you want to learn more about the technical details of all the methods, read the [Interpretable Machine Learning book]( https://christophm.github.io/interpretable-ml-book/agnostic.html).

![](/images/2018-04-30-interpretable-machine-learning-iml-and-mlr/iml-bear.jpg)

Let's explore the `iml`-toolbox for interpreting an `mlr` machine learning model with concrete examples!

## Data: Boston Housing

We'll use the `MASS::Boston` dataset to demonstrate the abilities of the iml package. This dataset contains median house values from Boston neighbourhoods. 

```{r}
data("Boston", package  = "MASS")
head(Boston)
```

## Fitting the machine learning model

First we train a randomForest to predict the Boston median housing value:

```{r, message = FALSE}
library("mlr")
data("Boston", package  = "MASS")

# create an mlr task and model
tsk = makeRegrTask(data = Boston, target = "medv")
lrn = makeLearner("regr.randomForest", ntree = 100)
mod = train(lrn, tsk)
```

## Using the iml Predictor container

We create a `Predictor` object, that holds the model and the data. The `iml` package uses R6 classes: New objects can be created by calling `Predictor$new()`.
`Predictor` works best with mlr models (`WrappedModel`-class), but it is also possible to use models from other packages.

```{r}
library("iml")
X = Boston[which(names(Boston) != "medv")]
predictor = Predictor$new(mod, data = X, y = Boston$medv)
```

## Feature importance

We can measure how important each feature was for the predictions with `FeatureImp`. The feature importance measure works by shuffling each feature and measuring how much the performance drops. For this regression task we choose to measure the loss in performance with the mean absolute error ('mae'); another choice would be the  mean squared error ('mse').


Once we created a new object of `FeatureImp`, the importance is automatically computed. 
We can call the `plot()` function of the object or look at the results in a data.frame.
```{r}
imp = FeatureImp$new(predictor, loss = "mae")
plot(imp)
imp$results
```

## Partial dependence

Besides learning which features were important, we are interested in how the features influence the predicted outcome. The `Partial` class implements partial dependence plots and individual conditional expectation curves. Each individual line represents the predictions (y-axis) for one data point when we change one of the features (e.g. 'lstat' on the x-axis). The highlighted line is the point-wise average of the individual lines and equals the partial dependence plot. The marks on the x-axis indicates the distribution of the 'lstat' feature, showing how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region).

```{r}
pdp.obj = Partial$new(predictor, feature = "lstat")
plot(pdp.obj)
```

If we want to compute the partial dependence curves for another feature, we can simply reset the feature.
Also, we can center the curves at a feature value of our choice, which makes it easier to see the trend of the curves:

```{r}
pdp.obj$set.feature("rm")
pdp.obj$center(min(Boston$rm))
plot(pdp.obj)
```

## Surrogate model

Another way to make the models more interpretable is to replace the black box with a simpler model - a decision tree. We take the predictions of the black box model (in our case the random forest) and train a decision tree on the original features and the predicted outcome. 
The plot shows the terminal nodes of the fitted tree.
The maxdepth parameter controls how deep the tree can grow and therefore how interpretable it is.

```{r}
tree = TreeSurrogate$new(predictor, maxdepth = 2)
plot(tree)
```

We can use the tree to make predictions:

```{r}
head(tree$predict(Boston))
```

## Explain single predictions with a local model

Global surrogate model can improve the understanding of the global model behaviour. 
We can also fit a model locally to understand an individual prediction better. The local model fitted by `LocalModel` is a linear regression model and the data points are weighted by how close they are to the data point for wich we want to explain the prediction.

```{r, message = FALSE}
lime.explain = LocalModel$new(predictor, x.interest = X[1,])
lime.explain$results
plot(lime.explain)
```

## Explain single predictions with game theory
An alternative for explaining individual predictions is a method from coalitional game theory named Shapley value.
Assume that for one data point, the feature values play a game together, in which they get the prediction as a payout. The Shapley value tells us how to fairly distribute the payout among the feature values.


```{r}
shapley = Shapley$new(predictor, x.interest = X[1,])
plot(shapley)
```

We can reuse the object to explain other data points:

```{r}
shapley$explain(x.interest = X[2,])
plot(shapley)
```

The results in data.frame form can be extracted like this:

```{r}
results = shapley$results
head(results)
```

The `iml` package is available on [CRAN](https://cran.r-project.org/web/packages/iml/index.html) and on [Github](https://github.com/christophM/iml).

<!--chapter:end:2018-04-30-interpretable-machine-learning-iml-and-mlr.Rmd-->

---
title: "Why R Conference"
authors: ["quay-au"]
date: 2018-07-05
categories: ["R"]
tags: ["conference", "why-r", "tutorial"]
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", warning = FALSE)
```

This July we had the great honor to present mlr and its ecosystem at the WhyR 2018 Conference in Wroclaw in Poland.
You can find the slides [here](https://github.com/mlr-org/mlr-outreach).
We want to thank the organizers for inviting us, providing us with great food and coffee and also many thanks all participants for showing great interest in mlr, 

We hope this could spark further improvement and new features as a result of the many interesting discussions we had with the participants.
This really encouraged us to work harder, in order to create an even better software.
We also hope this created interest in participating and contributing to our project, as the community really thrives on knowledge and experience of a very diverse set of people and backgrounds. 

![](/images/2018-07-05-whyr-conference/pic.jpg)




<!--chapter:end:2018-07-05-whyr-conference.Rmd-->

---
title: "Visualization of spatial cross-validation partitioning"
authors: ["patrick-schratz"]
date: 2018-07-25
categories: ["R", "r-bloggers"]
output:
  blogdown::html_page:
    toc: true
tags: ["spatial", "resampling", "spatial-CV", "rstats", "performance estimation"]
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library("mlr")
library("BBmisc")
library("ParamHelpers")
```

# Introduction

In July `mlr` got a new feature that extended the support for spatial data: The ability to visualize spatial partitions in cross-validation (CV) [9d4f3](https://github.com/mlr-org/mlr/commit/9d4f391d182f2a50ba532e91b59184adeb88ceb1).
When one uses the resampling descriptions "SpCV" or "SpRepCV" in `mlr`, the k-means clustering approach after Brenning (2005) is used to partition the dataset into equally sized, spatially disjoint subsets.
See also [this](https://www.r-spatial.org/r/2018/03/03/spatial-modeling-mlr.html) post on r-spatial.org and the [mlr vignette about spatial data](http://mlr.mlr-org.com/articles/handling_of_spatial_data.html) for more information.

# Visualization of partitions

When using random partitiong in a normal cross-validation, one is usually not interested in the random pattern of the datasets train/test split
However, for spatial data this information is important since it can help identifying problematic folds (ones that did not converge or showed a bad performance) and also proves that the k-means clustering algorithm did a good job on partitioning the dataset.

The new function to visualize these partitions in `mlr` is called `createSpatialResamplingPlots()`. 
It uses `ggplot2` and its new `geom_sf()` function to create spatial plots based on the resampling indices of a `resample()` object.
In this post we will use the examples of the function to demonstrate its use.

First, we create a resampling description `rdesc` using spatial partitioning with five folds and repeat it 4 times.
This `rdesc` object is put into a `resample()` call together with our example task for spatial matters, `spatial.task`.
Finally, we use the `classif.qda` learner to have a quick model fit.

```{r, results='hide', message=FALSE}
library(mlr)
rdesc = makeResampleDesc("SpRepCV", folds = 5, reps = 4)
resamp = resample(makeLearner("classif.qda"), spatial.task, rdesc)
```

Now we can use `createSpatialResamplingPlots()` to automatically create one plot for each fold of the `resamp` object.
Usually we do not want to plot all repetitions of the CV.
We can restrict the number of repetitions in the argument `repetitions`.

Besides the required arguments `task` and `resample`, the user can specifiy the coordinate reference system that should be used for the plots.
Here it is important to set the correct EPSG number in argument `crs` to receive accurate spatial plots.
In the background, `geom_sf()` (more specifically `coords_sf()`) will transform the CRS on the fly to EPSG: 4326.
This is done because lat/lon reference systems are better for plotting as UTM coordinates usually clutter the axis.
However, if you insist on using UTM projection instead of WGS84 (EPSG: 4326) you can set the EPSG code of your choice in argument `datum`.

```{r}
plots = createSpatialResamplingPlots(spatial.task, resamp, crs = 32717,
  repetitions = 2, x.axis.breaks = c(-79.065, -79.085),
  y.axis.breaks = c(-3.970, -4))
```

To avoid overlapping axis breaks you might want to set the axis breaks on your own as we did here.

Now we got a list of 2L back from `createSpatialResamplingPlots()`. 
In the first list we got all the plots, one for each fold.
Since we used two repetitions and five folds, we have a total of ten instances in it.

The second list consists of labels for each plot.
These are automatically created by `createSpatialResamplingPlots()` and can serve as titles later on.
Note that for now we just created the `ggplot` objects (stored in the first list of the `plots` object).
We still need to plot them!

Single `ggplot` objects can be plotted by just extracting a certain object from the list, e.g. `plots[[1]][[3]]`.
This would plot fold #3 of repetition one.

```{r, fig.align='center', warning=FALSE}
plots[[1]][[3]]
```
However usually we want to visualize all plots in a grid.
For this purpose we highly recommend to use the `cowplot` package and its function `plot_grid()`.

The returned objects of `createSpatialResamplingPlots()` are already tailored to be used with this function.
We just need to hand over the list of plots and (optional) the labels:

```{r, fig.align='center', fig.width=10}
cowplot::plot_grid(plotlist = plots[["Plots"]], ncol = 5, nrow = 2,
  labels = plots[["Labels"]])
```

# Multiple resample objects

`createSpatialResamplingPlots()` is quite powerful and can also take multiple `resample()` objects as inputs with the aim to compare both.
A typical use case is to visualize the differences between spatial and non-spatial partitioning.

To do so, we first create two `resample()` objects (one using "SpRepCV", one using "RepCV"):

```{r, results='hide', message=FALSE}
rdesc1 = makeResampleDesc("SpRepCV", folds = 5, reps = 4)
r1 = resample(makeLearner("classif.qda"), spatial.task, rdesc1)
rdesc2 = makeResampleDesc("RepCV", folds = 5, reps = 4)
r2 = resample(makeLearner("classif.qda"), spatial.task, rdesc2)
```

Now we can hand over both objects using a named list. 
This way the list names will also directly be used as a prefix in the resulting plot labels.

```{r, fig.align='center', fig.width=10}
plots = createSpatialResamplingPlots(spatial.task,
  list("SpRepCV" = r1, "RepCV" = r2), crs = 32717, repetitions = 1,
  x.axis.breaks = c(-79.055, -79.085), y.axis.breaks = c(-3.975, -4))

cowplot::plot_grid(plotlist = plots[["Plots"]], ncol = 5, nrow = 2,
  labels = plots[["Labels"]])
```

# References

Brenning, A. (2012). Spatial cross-validation and bootstrap for the assessment of prediction rules in remote sensing: The R package sperrorest. In 2012 IEEE International Geoscience and Remote Sensing Symposium. IEEE. https://doi.org/10.1109/igarss.2012.6352393

<!--chapter:end:2018-07-25-visualize-spatial-cv.Rmd-->

---
title: mlr-2.14.0
authors: ["patrick-schratz"]
date: '2019-04-18'
slug: mlr-2-14-0
output:
  blogdown::html_page:
    toc: true
categories:
  - r-bloggers
tags:
  - release
---

The last _mlr_ release was in August 2018 - so it was definitely time for a new release after around 9 months of development!

The NEWS file can be found directly [here](https://mlr.mlr-org.com/news/index.html).

In this post we highlight some of the new implementations that come along with this release of v2.14.0

## Filters

We integrated the filter methods from the _praznik_ package. These were quite few:

- praznik_JMI
- praznik_DISR
- praznik_JMIM
- praznik_MIM
- praznik_NJMIM
- praznik_MRMR
- praznik_CMIM

Also, a long awaited PR that we finally merged was the inclusion of the _FSelectorRcpp_ filters.
These are around 100 times faster than the Java-driven ones from the _FSelector_ package.

In addition, we are now using a consistent naming scheme for the filters following `<package-name>_<filter-name>`.
This change might break your existing code if you used _mlr_ filters before.
However, since it is just a naming change we think the burden of updating your code is acceptable.

## Learners

Two new learners were added:

- classif.liquidSVM 
- regr.liquidSVM

Learner `regr.h2o.gbm` now uses `h2o.use.data.table = TRUE` by default which should result in a runtime performance increase.

It is also possible to retrieve the feature importance of h2O learners now.

## Resampling

You can now provide fully predefined indices for resampling.
This is useful for datasets that have a certain grouping structure (e.g. spatial data) that is difficult to specify otherwise. 

## mlr-org NEWS

You might be wondering what we've been up to in the last months in our group.
The major project that we started was [mlr3](https://github.com/mlr-org/mlr3).
This is a clean rewrite of _mlr_ with a modular structure to simplify usage and maintenance of the "mlr idea" in the future, both for users and developers.
We are not completely finished yet, but you can take a look at the Github repo at what we have achieved so far.
Once we are ready to release the initial version, we will of course write a dedicated post about it.

Putting a lot of time into _mlr3_ means having less time for responding to issues and questions in _mlr_.
We would like to apologize for this.
We are working on this more or less as a side project along our day jobs and our resources are limited.
If you want to help and get involved with _mlr_ or _mlr3_, we would be very happy to have you.
Our team is not a closed group and anyone can contribute to the mlr-org projects.

The change in development focus also led to a change of maintainer for _mlr_.
As [Bernd Bischl](https://www.compstat.statistik.uni-muenchen.de/people/bischl/) (the creator and maintainer) of _mlr_ has a lot of duties, we decided to make [Lars Kotthoff](https://www.cs.uwyo.edu/~larsko/) and [Patrick Schratz](https://pjs-web.de/) the new maintainers of the _mlr_ package.

_mlr_ will only get bug fixes and minor updates, as we are focusing the development of new things on _mlr3_.
Right now, we have over 400 issues and 30 pull requests so there is a still a lot to do :)

## Roadmap for mlr

We are will publish new releases every three months from now on, regardless of the changes that have come in.
_mlr_ will continue to exist next to _mlr3_.
If users start contributing new features to _mlr_, we are also happy to include those in the package.
As announced already, we will clean up the _mlr_ repo issue and pull request in the coming months to be able to fully concentrate on _mlr3_ after its initial release. 

<!--chapter:end:2019-04-18-mlr-2-14-0.Rmd-->

---
title: "mlr + drake: Reproducible machine-learning workflow management"
authors: ["patrick-schratz"]
date: '2019-05-06'
slug: mlr-example-for-drake
output:
  blogdown::html_page:
    toc: true
categories:
  - r-bloggers
tags:
  - reproducibility
  - make
  - workflow
  - drake
  - mlr
---

You may have heard about the [drake](https://ropensci.github.io/drake/) package.
It got a lot attention recently in the R community because it simplifies reproducible workflow management.
This comes especially handy for large projects which have hundreds of intermediate steps.
Built-in High-Performance-Cluster (HPC) support and graph visualization are just two goodies that come on top of the basic functionality.

_drake_ is able to track changes in your intermediate targets.
This means once you change something in your early workflow pipeline, _drake_ will automatically update all follow-up objects that might be affected by this change.
The following tweet wraps the struggle of keeping track of dependencies in a research project in an simple picture:

<blockquote class="twitter-tweet" data-conversation="none" data-lang="de"><p lang="en" dir="ltr">Save me from myself and having to remember all this when files change <a href="https://t.co/hVeSFQOimj">pic.twitter.com/hVeSFQOimj</a></p>&mdash; Dr. Brianna McHorse 🏳️‍🌈 (@fossilosophy) <a href="https://twitter.com/fossilosophy/status/966408174470299648?ref_src=twsrc%5Etfw">21. Februar 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

The maintainer of _drake_, Will Landau ([\@wlandau](https://github.com/wlandau-lilly)) is extremely responsive and has also written one of the most extensive and detailed [manuals](https://ropenscilabs.github.io/drake-manual/) that exist in the R package jungle.

If you have installed _drake_, you can start right away with one of the built-in examples.

```r
drake::drake_example("mlr-slurm")
```

At the time of writing, there are 17(!) [examples](https://github.com/wlandau/drake-examples) that you can choose from.
One of the newest shows how to use _mlr_ with _drake_ on a HPC.

Machine-Learning projects/tasks interact especially well with the _drake_ idea since you can easily create [large comparison matrices](https://ropenscilabs.github.io/drake-manual/plans.html#large-plan) using different algorithms / hyperparameter settings.
At the same time _drake_ can sent these settings in parallel to a HPC for you, simplifying your modeling tasks a lot.

<div style="text-align:center"><img width="100px" src ="../../images/2019-05-06-mlr-example-for-drake/drake.svg" /></div>

<!--chapter:end:2019-05-06-mlr-example-for-drake.Rmd-->

---
title: "Introducing mlrPlayground"
authors: ["sebastian-gruber"]
date: "2019-08-12"
output:
  blogdown::html_page:
    toc: true
categories:
  - r-bloggers
tags:
  - shiny
  - visualization
  - mlr
  - teaching
  - classification
  - regression
  - hyperparameter
---

![](/images/2019-06-23-introducing-mlrplayground/07_welcome.gif)

<br/>

## First of all

You may ask yourself how is this name ‘mlrPlayground’ even justified?
What a person dares to put two such opposite terms in a single word and expects people to take him seriously?

I assume most of you know ‘mlr’, for those who don’t: It is a framework offering a huge variety of tools for simplifying machine learning tasks in R.
Quite the opposite from a place, where you can play with your best friends, make new friends, live out your fantasies and just have a great time the whole day until your parents pick you up.
Well, for most of the readers here this may not be the case anymore – we know, we are still young in our heart, but let’s be honest … 
For sure, we all have those memories and definitely have certain associations with the word ‘Playground’.
So, what is about this thing called ‘mlrPlayground’?
<br/>

## The idea

The idea behind this project was to offer a platform in the form of a <a href="https://shiny.rstudio.com/">Shiny</a> web application, in which a user can try out different kinds of learners provided by the mlr package.
On a small set of distinct and tunable regression and classification tasks, it is possible to observe the prediction/performance behavior based on changes on the task, the learner or the learner's hyperparameters.
The user is able to gain new insights and a deeper understanding of how a learner performs, where it’s advantages are and in which cases the learner might fail.

There are a lot of different settings we want to offer in the user interface, and so – to not remove the fun of our playground – a huge effort went into creating an aesthetically pleasing and user-friendly UI.
To achieve this, a website template was downloaded from <a href="https://templated.co/">Templated</a> and used as the baseline design. 
After extending the template with missing CSS classes, most of the used shiny widgets have been overwritten – or even completely replaced –, offering refreshingly new visuals for the well-known shiny framework.
For the smooth feel of the app, an object-oriented <a href="https://cran.r-project.org/web/packages/R6/index.html">R6</a> class system with reactive attributes was engineered for the backend to give a well-defined framework of which elements should trigger what evaluation; an otherwise extremely tiresome and error-prone task for dozens of different UI elements.

After all ‘mlrPlayground’ may not be as fun as a real playground, but you are also not as likely to hurt yourself and it is definitely more entertaining than looking at boring pictures of learners in a book.
<br/>

## The features

In the following, an extended overview of most available features is presented with the help of animated pictures.

##### Task selection

When running the app and scrolling down you can basically see two big boxes with content: The left one is for the task and learner settings, the right one for an overview of the learner’s behavior.
For first-time users, it is advisable (but not necessary) to explore the different tasks.
After finding the right button (this task is left to the motivated reader) a panel with all available datasets is extended over our screen.
Here, we can choose the task type to receive a varied selection of classification or regression tasks and several other parameters influencing each dataset such as size, noise, and the train/test split.

![](/images/2019-06-23-introducing-mlrplayground/06_tasks.gif)
<br/>

##### Learner selection

Back at the main panel, we can then select a learner for our task – due to limited manpower not all available learners in _mlr_ made it into this selection.
You can expect additions in the future.
By making a selection, the main process of the app, consisting of several substeps, gets executed:

<ul>
<li><p>a model is trained on the training set of the defined task</p></li>
<li><p>performance measures are calculated on the corresponding test set</p></li>
<li><p>the prediction surface is evaluated and depicted in an interactive plot.</p></li>
</ul>

![](/images/2019-06-23-introducing-mlrplayground/01_learner.gif)
<br/>

##### Performance measures

The first visual change after calculations have finished is the appearance of the prediction plot. 
Besides, there are now a whole bunch of different performance measures to explore by clicking on the equally named bar on the right-hand side. 
These provide the opportunity to manually tune each learner on the tasks or compare how each measure behaves in certain scenarios. 
You will not only be able to understand the learner better, but also each measure. 
What's particularly interesting for the latter is the configurable ratio of train and test data in the task selection window.

![](/images/2019-06-23-introducing-mlrplayground/02_measures.gif)
<br/>

##### Hyperparameters

If you want to see the effect of hyperparameters on the plot/performance measures, simply click on the small cog next to the learner selection. 
Now, the left panel is replaced by a window with all available hyperparameters – changes here will have an immediate effect: The whole model will be rerun and all plots/numbers will be updated.

![](/images/2019-06-23-introducing-mlrplayground/03_parameters.gif)

###### Second learner

Only observing changes of a single learner may become a bit boring after time, so an option is available to easily double the fun: Pressing “Add learner” gives the choice of adding a second learner with a second plot, making it possible to compare the prediction surfaces of two different learners on the same task right next of each other. 
Exciting!

![](/images/2019-06-23-introducing-mlrplayground/05_sndlearner.gif)
<br/>

But wait, there’s still more to offer!

<br>

##### Learning curve and ROC

The user can change the created plots on the right panel by switching to the tabs “Learning Curve” (classification and regression) or “ROC” (only classification). 
The former tells you how well the learner performed or how multiple learners compare to each other.
All of this happens with respect to the user selected performance measures (y-axis) on a fraction of the original train data (x-axis). 
Every additionally selected measure results in an additional plot. 

Last but not least, the ROC curve is only a ROC curve with its default settings for the x- and y-axis: Plotting the true positive rate (TPR) against the false positive rate (FPR).
It shows how well the learner can separate the classes (the bigger the area under the curve, the better).

![](/images/2019-06-23-introducing-mlrplayground/04_plots.gif)
<br/>

## Usage

If you want to explore _mlrPlayground_ on your own, you have two possibilities:

Either visit the [website](https://compstat-lmu.shinyapps.io/mlrPlayground/) (big thanks to the Department of computational statistics at the LMU Munich for hosting)

or

install it locally on your own machine using `remotes::install_github("SebGruber1996/mlrPlayground")`
and start with `mlrPlayground::start()`.
The first approach is quicker but may crash if too many people access it at the same time, while the latter requires a local installation.
It's your choice. 
<br/>

Thanks for spending your time reading this blog post instead of being on a real playground!
<br/>

Side note: The app is still far from being mature.
The UI is -- especially for the hyperparameters -- not working well on small screens. 
Also, some hyperparameter settings may cause the app to crash.
Even if these things would be solved, there are still a lot of learners missing in the app which are available in the _mlr_ package.
Pull requests are welcome at https://github.com/SebGruber1996/mlrPlayground.

<!--chapter:end:2019-06-23-introducing-mlrplayground.Rmd-->

---
title: mlr3-0.1.0
authors: ["patrick-schratz"]
date: '2019-07-31'
slug: mlr3-0-1-0
output:
  blogdown::html_page:
    toc: true
categories:
  - R
  - r-bloggers
tags:
  - machine-learning
  - mlr3
  - release
  - framework
  - R
---

# mlr3 - Initial release

The [mlr-org team](https://www.notion.so/mlrorg/Developer-Team-and-Contributors-e2a4b78aee1f43a8b830c2ce419e68a9) is very proud to present the initial release of the [mlr3 machine-learning framework](https://github.com/mlr-org/mlr3) for R.

[mlr3](https://github.com/mlr-org/mlr3) comes with a clean object-oriented-design using the [R6](https://github.com/r-lib/R6) class system.
With this, it overcomes the limitations of R's S3 classes.
It is a rewrite of the well-known [_mlr_](https://github.com/mlr-org/mlr) package which provides a convenient way of accessing many algorithms in R through a consistent interface.

While [_mlr_](https://github.com/mlr-org/mlr) was one big package that included everything starting from preprocessing, tuning, feature-selection to visualization, [mlr3](https://github.com/mlr-org/mlr3) is a **package framework** consisting of many packages containing specific parts of the functionality required for a complete machine-learning workflow.

# Background - why a rewrite?

The addition of many features to [_mlr_](https://github.com/mlr-org/mlr) has led to a "feature creep" which makes it hard to maintain and extend.  

Due to the many tests in [_mlr_](https://github.com/mlr-org/mlr), a full CI run of the package on Travis CI takes more than 30 minutes.
This does not include the installation of dependencies, running _R CMD check_ or building the pkgdown site (which includes more than 20 vignettes with executable code).
The dependency installation alone takes 1 hour(!).
This is due to the huge number of packages that _mlr_ imports to be able to use all the algorithms for which it provides a unified interface.
On a vanilla CI build without cache there woulöd be 326(!) packages to be installed.

_mlr_ consists of roughly 40k lines of R code.

```
github.com/AlDanial/cloc v 1.82  T=0.98 s (1335.1 files/s, 226920.4 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
HTML                           346          16592           6276         125687
R                              878           5909          12566          40696
Rmd                             44           2390           5296           2500
Markdown                        11            196              0           1136
XML                              1              0              0            963
YAML                             4             14              5            244
CSS                              2             69             66            203
C                                3             16             33            106
JSON                             5              0              0             98
JavaScript                       2             21              9             80
Bourne Shell                     4              8              6             69
C/C++ Header                     1              7              0             20
SVG                              1              0              1             11
-------------------------------------------------------------------------------
SUM:                          1302          25222          24258         171813
-------------------------------------------------------------------------------
```

Adding any feature has become a huge pain since it requires passing down objects through many functions.
In addition, a change to one part of the project might trigger unwanted side-effects at other places or even break functions.
Even though most of the package is covered by tests, this creates a high wall of reluctance to extend the package further.

After mlr has become more prominent, many people started opening issues and pull requests (which is great!). 
While the package authors and its creator (Bernd Bischl) were able to cope with the amount of user input in the beginning, the number of open issues increased steadily in recent years.
Since nobody has been specifically paid for _mlr_ in the past, there was just not enough man power to be able to handle this amount of questions and feature requests in the past.
The issue tracker exceeded 400 open issues recently and people often got no reply for their pull requests in a reasonable amount of time.
At the same time, the core team could invest less time into _mlr_ due to other new responsibilities in their professional lifes.

_mlr_ was built using the S3 class system.
It functionality relied partyl on packages that were also maintained by the mlr-team (e.g. _BBmisc_, _parallelMap_).
Maintaining so many small packages for a little benefit took away a lot of time.
However, back in the days these actions were needed since many convient packages and functions were missing at that point in time.

The mlr-team decided that is was time to change something.
Since so many people like the _mlr_ package and its philosophy, we decided to make a fresh start.
Taking into account all the problematic points that we learned from in the last years:

- The limitation of the S3 class system
- The fear of a "feature-creep" in one single package
- Long CI checking times
- The burden of maintaining too many parts of a "framework" package yourself

# The new [mlr3](https://github.com/mlr-org/mlr3) package framework

- Uses the new R6 object-oriented class system
- Splits functionalities into extension packages; only the core functionality is provided via package _mlr3_
- Outsources parts of the functionality to external packages (parallelization -> future, measures -> Metrics)
- Documentation is written in bookdown format which is searchable
- Extension packages are assigned to specific persons, reducing reply-times to user requests
- comes with a new package _mlr3pipelines_ (the successor of mlrCPO) providing smart operators for creating a complete machine-learning workflow using the piping approach
- uses _data.table_ under the hood to speed up internal computations

_mlr3_ is openly developed on Github for around one year now.
Not all functionality of the _mlr_ package is already re-implemented.
In fact, this release can be seen as a beta version.
We're happy to take user feedback to improve _mlr3_ in the next months.

The following list shows only a subset of the mlr3 package framework.
For a full list, please visit our [wiki](https://github.com/mlr-org/mlr3/wiki/Extension-Packages).
The scope of most packages should be clear simply by their name.
For more information, visit the respective Github repos of the packages by clicking on the package name.

| Package                                                   | Status                                                                                                                  |
| --------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| [mlr3](https://github.com/mlr-org/mlr3)                   | [![Travis](https://travis-ci.org/mlr-org/mlr3.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3)                   |
| [mlr3book](https://github.com/mlr-org/mlr3book)           | [![Travis](https://travis-ci.org/mlr-org/mlr3book.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3book)           |
| [mlr3db](https://github.com/mlr-org/mlr3db)               | [![Travis](https://travis-ci.org/mlr-org/mlr3db.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3db)               |
| [mlr3filters](https://github.com/mlr-org/mlr3featsel)     | [![Travis](https://travis-ci.org/mlr-org/mlr3filters.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3filters)     |
| [mlr3learners](https://github.com/mlr-org/mlr3learners)   | [![Travis](https://travis-ci.org/mlr-org/mlr3learners.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3learners)   |
| [mlr3misc](https://github.com/mlr-org/mlr3misc)           | [![Travis](https://travis-ci.org/mlr-org/mlr3misc.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3misc)           |
| [mlr3ordinal](https://github.com/mlr-org/mlr3ordinal)     | [![Travis](https://travis-ci.org/mlr-org/mlr3ordinal.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3ordinal)     |
| [mlr3pipelines](https://github.com/mlr-org/mlr3pipelines) | [![Travis](https://travis-ci.org/mlr-org/mlr3pipelines.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3pipelines) |
| [mlr3spatiotemporal](https://github.com/mlr-org/mlr3spatiotemporal) | [![Travis](https://travis-ci.org/mlr-org/mlr3spatiotemporal.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3spatiotemporal) |
| [mlr3survival](https://github.com/mlr-org/mlr3survival)   | [![Travis](https://travis-ci.org/mlr-org/mlr3survival.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3survival)   |
| [mlr3tuning](https://github.com/mlr-org/mlr3tuning)       | [![Travis](https://travis-ci.org/mlr-org/mlr3tuning.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3tuning)       |
| [mlr3viz](https://github.com/mlr-org/mlr3viz)             | [![Travis](https://travis-ci.org/mlr-org/mlr3viz.svg?branch=master)](https://travis-ci.org/mlr-org/mlr3viz)             |
| [paradox](https://github.com/mlr-org/paradox)             | [![Travis](https://travis-ci.org/mlr-org/paradox.svg?branch=master)](https://travis-ci.org/mlr-org/paradox)             |

We also started to write a [book](https://mlr3book.mlr-org.com/) about _mlr3_ which aims to serve both as a reference book for machine learning in general and as a manual for _mlr3_ specifically.

# [mlr3](https://github.com/mlr-org/mlr3) at useR!2019

The mlr-org team gave two dedicated talks to present the new mlr3 at the useR! conference in Toulouse.

The recorded videos will be inserted here once they are available.

<!--chapter:end:2019-07-29-mlr3-0-1-0.Rmd-->

---
title: mlr-2.15.0
authors: ["patrick-schratz"]
date: '2019-08-06'
slug: mlr-2-15-0
bibliography: ../../static/bib/mlr-2-15-0.bib
categories:
  - R
  - r-bloggers
tags:
  - mlr
  - release
output:
  blogdown::html_page:
    toc: true
---

We just released _mlr_ v2.15.0 to CRAN. 
This version includes some breaking changes and the usual bug fixes from the last three months.

We made good progress on the goal of cleaning up the [Github repo](https://github.com/mlr-org/mlr).
We processed nearly all open pull requests (around 40).
In the next months we will focus on cleaning up the issue tracker even though most of our time will go into improving the successor package [mlr3](https://github.com/mlr-org/mlr3) and its extension packages.

Unless there are active contributions from the user side, we do not expect much feature additions for the next version(s) of _mlr_.

# Changes to `benchmark()`

The `benchmark()` function does not store the tuning results (stored in the `$extract` slot) anymore by default.
This change was made to prevent BenchmarkResult (BMR) objects from getting huge in size (~ GB) when multiple models are compared with extensive tuning.
Unless you want to do a analysis on the tuning effects, you do not need the tuning results to compare the performance of the algorithms. 
Huge BMR objects can cause various troubles.
One of them (which was the inital root for this change) appears when benchmarking is done on a HPC using multiple workers.
Each worker has a limited amount of memory and expecting a huge BMR might limit the amount of workers that can be spawned.
In addition, loading the large resulting BMR into the global environment (or merging it using `mergeBenchmarkResults()`) for post-analysis will become a pain.
To save users from all of these troubles in the first place, we decided to change the default.

To store the tuning results, you have to actively set `keep.extract = TRUE` from now on.
Not storing the tuning was actually already implicitly the default in `resample()` since the user had to set the `extract` argument manually to save certain results (tuning, feature importance).
With the new change the package became more consistent.

# Changes to Filters

## New ensemble filters

With this release it is possible to calculate ensemble filters with _mlr_ [@seijo-pardo2017].
"Ensemble filters" are similar to ensemble models in the way that multiple filters are used to generate the ranking of features.
Multiple aggregations functions are supported (`min()`, `mean()`, `median()`, "Borda") with the latter being the most used one in literature while writing this.

To our knowledge there is no other package/framework in R currently that supports ensemble filters in a similar way _mlr_ does.
Since _mlr_ makes it possible to use filters from a [variety of different packages](https://mlr.mlr-org.com/articles/tutorial/filter_methods.html), the user is able to create powerful ensemble filters.
Note however that currently you cannot tune the selection of simple filters since tuning a character vector param is not supported by _ParamHelpers_. 
See [this discussion](https://github.com/berndbischl/ParamHelpers/pull/206) for more information.

Here is a simple toy example how to create ensemble filters in _mlr_ from `?filterFeatures()`:

```{r, eval = TRUE, collapse=TRUE}
library(mlr)
filterFeatures(iris.task, method = "E-min",
  base.methods = c("FSelectorRcpp_gain.ratio", "FSelectorRcpp_information.gain"), abs = 2)
```

## New return structure for filter values

With the added support for ensemble filters we also changes the return structure of calculated filter values.

The new makes it easier to apply post-analysis tasks like grouping and filtering.
The "method" of each row is now grouped into one column and the filter values are stored in a separate one.
We also added a default sorting of the results by the "value" of each "method".

Below is a comparison of the old and new output:

```{r, collapse=TRUE}
# new
generateFilterValuesData(iris.task,
  method = c("FSelectorRcpp_gain.ratio", "FSelectorRcpp_information.gain"))
```

```{r, eval = FALSE}
# old
generateFilterValuesData(iris.task,
  method = c('gain.ratio','information.gain')
## FilterValues:
## Task: iris-example
##           name    type gain.ratio information.gain
## 1 Sepal.Length numeric  0.4196464        0.4521286
## 2  Sepal.Width numeric  0.2472972        0.2672750
## 3 Petal.Length numeric  0.8584937        0.9402853
## 4  Petal.Width numeric  0.8713692        0.9554360
```

# Learners

Besides the integration of new learners and some added options for integrated ones (check the [NEWS](https://mlr.mlr-org.com/news/index.html#learners---general) file), we fixed a bug that caused an incorrect aggregation of probabilities in certain cases.
This bug was around undetected for quite some time and was revealed due to a change in _data.table_'s `rbindlist()` function.
Thankfully [\@danielhorn](https://github.com/danielhorn) reported this [issue](https://github.com/mlr-org/mlr/issues/2578) and we could fix it within a few days.

Another mentionable change is that the commonly used  `e1071::svm()` learner now only uses the formula interface internally if factors are present in the data.
This aims to prevent ["stack overflow" problems](https://github.com/mlr-org/mlr/issues/1738) that some user encountered with large datasets.

With [PR #1784](https://github.com/mlr-org/mlr/issues/1784) we added more support for estimating standard errors using the internal methods of the "Random Forest" algorithm. 
Please check the [NEWS](https://mlr.mlr-org.com/news/index.html#learners---general) file for more detailed information about the implemented RF learners.

# References

<!--chapter:end:2019-08-06-mlr-2-15-0.Rmd-->

---
title: mlr wins Open Source Machine Learning Software Award
authors: ["lars-kotthoff"]
date: '2019-12-21'
slug: mlr wins Open Source Machine Learning Software Award
categories:
  - R
  - r-bloggers
tags:
  - mlr
output:
  blogdown::html_page:
    toc: true
---

# mlr receives Open Source Machine Learning Project Award

![](/images/2019-12-21-mlr-award/award1.jpg)

We're extremely proud to have received the Open Source Machine Learning Project Award at [ODSC West 2019](https://odsc.com/california/).
We were joined by [Tensorflow](https://github.com/tensorflow/tensorflow), [DataKind](https://www.datakind.org/), and [SHAP](https://github.com/slundberg/shap), which also received awards.

The ODSC awards recognize projects that have made sustained contributions to open data science.
The mlr project has been active for more than 8 years now, being sustained by the contributions of individual volunteers.
We are especially honored to have been recognized like this as the other awarded projects are coordinated and run by companies with paid staff -- this speaks to the dedication of everybody who has made mlr possible.

Lars Kotthoff received the award on behalf of the entire mlr team; you can find his presentation slides giving more background on the project [here](https://github.com/mlr-org/mlr-outreach/blob/master/2019_odsc/award-talk.pdf).
After the award talk, Martin Binder gave a tutorial on mlr3.
You can find his slides and materials [here](https://github.com/mlr-org/mlr-outreach/tree/master/2019_odsc).

![](/images/2019-12-21-mlr-award/award2.jpg)

More information on the awards can be found [here](https://medium.com/@ODSC/announcing-the-odsc-west-2019-data-science-award-winners-e08269c7f293).

<!--chapter:end:2019-12-21-mlr-award.Rmd-->

---
title: mlr3 tutorial on useR!2020muc
authors: ["michel-lang"]
date: '2020-03-06'
slug: mlr3 tutorial user2020muc
categories:
  - R
  - r-bloggers
tags:
  - mlr3
  - mlr3pipelines
  - mlr3tuning
  - tutorial
  - user2020
  - user2020muc
output:
  blogdown::html_page:
    toc: true
---

# mlr3 tutorial at the useR!2020 European hub

We are thrilled that we got accepted for a tutorial at the [useR!2020 satellite event in Munich](https://user2020muc.r-project.org/) on July 7th.
[Bernd Bischl](https://twitter.com/BBischl) and [Michel Lang](https://twitter.com/michellangts) will give an introduction to [mlr3](https://mlr3.mlr-org.com), the successor of the [mlr](https://mlr3.mlr-org.com) package for machine learning in R.


The main objective of the tutorial is to introduce and familiarize users with mlr3 and its ecosystem of extension packages.
This will allow participants to take advantage of its functionality for their own projects, in particular:

* how to benchmark and compare different machine learning approaches in a statistically sound manner,
* how to build complex machine learning workflows with [mlr3pipelines](https://mlr3pipelines.mlr-org.com/), including preprocessing and stacked ensembles,
* automatic hyperparameter tuning and pipeline optimization (AutoML) with [mlr3tuning](https://mlr3tuning.mlr-org.com/),
* how to get the technical "nitty-gritties" for machine learning experiments right, e.g., speed up by parallelization, encapsulation of experiments in external processes or working on databases.

After the tutorial, participants will be able to implement complex solutions to real-world machine learning problems, and to evaluate the different design decisions necessary for such implementations, in particular the choice between different modelling and preprocessing techniques.


# About the useR!2020 satellite event in Munich

The conference is a satellite event of the "official" [useR!2020 in St. Louis](https://user2020.r-project.org/), USA.
It is actively supported by the [R Foundation](https://www.r-project.org/foundation/), and there will be streamed keynote talks from the US to Europe and vice versa.
The following speakers will give a keynote in Munich (you can find the tentative programme [here](https://user2020muc.r-project.org/programme/)):

* Kurt Hornik and Uwe Ligges about the future of [CRAN](https://cran.r-project.org/)
* Anna Krystalli about computational reproducibility
* Przemysław Biecek about explorable and explainable machine learning models

Note that there is still time to [submit an abstract](https://user2020muc.r-project.org/participation/) for a talk, a lightning talk or a poster.
Registration is also open now!

<!--chapter:end:2020-03-06-mlr3-tutorial-user2020.Rmd-->

---
title: "Explainable machine learning with mlr3 and DALEX"
authors: ["michel-lang"]
date: '2020-03-30'
slug: mlr3 loves dalex
categories:
  - R
  - r-bloggers
tags:
  - mlr3
  - mlr3book
  - dalex
output:
  blogdown::html_page:
    toc: true
---

[Przemysław Biecek](https://github.com/pbiecek) and [Szymon Maksymiuk](https://github.com/maksymiuks) added a [new chapter](https://mlr3book.mlr-org.com/interpretability-dalex.html) to the [mlr3 book](https://mlr3book.mlr-org.com/) on how to analyze machine learning models fitted with [mlr3](https://cran.r-project.org/package=mlr3) using the excellent [DALEX package](https://cran.r-project.org/package=DALEX).

The contributed chapter covers an analysis of a random regression forest (implemented in the [ranger package](https://cran.r-project.org/package=ranger)) on data extracted from the FIFA video game.
In more detail, the following methods for explainable machine learning are showcased:

* Dataset level exploration: Feature importance and Partial dependency plots.
* Instance level explanation: Break Down, SHapley Additive exPlanations (SHAP), and Ceteris Paribus plots.

Here is a small preview illustrating the effect of different features on the monetary value of Cristiano Ronaldo:

![](/images/2020-03-30-dalex/shap.svg)

Read the complete chapter [here](https://mlr3book.mlr-org.com/interpretability-dalex.html).

<!--chapter:end:2020-03-30-dalex.Rmd-->

---
title: "Webinar on mlr3pipelines"
authors: ["jakob-richter"]
date: '2020-05-20'
slug: mlr3pipelines webinar
categories:
  - R
  - r-bloggers
tags:
  - mlr3
  - mlr3pipelines
output:
  blogdown::html_page:
    toc: true
---

We are happy to be invited to the [WhyR Webinar Series](http://whyr.pl/foundation/webinars/) to present you with some insights into [`mlr3pipelines`](https://mlr3pipelines.mlr-org.com/). 

The authors of `mlr3pipelines`, [Bernd Bischl](https://www.compstat.statistik.uni-muenchen.de/people/bischl/), [Martin Binder](https://www.compstat.statistik.uni-muenchen.de/people/binder/) and [Florian Pfisterer](https://www.compstat.statistik.uni-muenchen.de/people/pfisterer/) will give you a detailed overview of the vast capabilities of `mlr3pipelines`, which includes basic data preprocessing operations (like PCA, filtering etc.) but also advanced learner operations, that enable you to build you own stacked learners or ensembles.
Combined with tuning methods from [`mlr3tuning`](https://mlr3pipelines.mlr-org.com/) you can build tunable pipelines which effectively gives you the ability to construct AutoML-pipelines. 

The webinar will be live on the 28th May at 20:00 (CEST / CST+2) on [YouTube](https://www.youtube.com/embed/4r8K3GO5wk4).

Make sure to [add this event to your calendar](http://www.google.com/calendar/event?action=TEMPLATE&dates=20200528T200000Z%2F20200528T220000Z&text=mlr3pipelines%20Webinar&location=&details=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D4r8K3GO5wk4) or set the reminder on YouTube. 

<iframe width="800" height="450" src="https://www.youtube.com/embed/4r8K3GO5wk4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



We will update this post once the slides are available.

<!--chapter:end:2020-05-20-mlr3pipelines-webinar.Rmd-->

---
title: "useR 2020 tutorial on mlr3, mlr3tuning and pipelines"
author: ["marc-becker"]
date: 2020-08-04
categories: ["R"]
tags: ["workshop"]
---

  ```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

![](/images/2020-08-04-useR-2020-tutorial/head.jpeg)

We would like to invite you to our useR 2020 tutorial on `mlr3`, `mlr3tuning` and `mlr3pipelines` taught by Bernd Bischl and Michel Lang. 
The tutorial will take place on 7th August at 10:00 (UTC-5).
You can find more information and the registration link on [meetup.com](https://www.meetup.com/rladies-galapagos-islands/events/272240650/?isFirstPublish=true).





<!--chapter:end:2020-08-04-useR-2020-tutorial.Rmd-->

---
title: 'Introducing mlr3cluster: Cluster Analysis Package'
author: Damir Pulatov
date: '2020-08-14'
slug: mlr3cluster,tutorial,cluster
categories: 
  - R
tags:
  - mlr3
  - mlr3cluster
  - cluster analysis
  - tutorial
output:
blogdown::html_page:
  toc: true
---

Tired of learning to use multiple packages to access clustering algorithms? 

Using different packages makes it difficult to compare the performance of 
clusterers?

It would be great to have just one package that makes interfacing 
all things clustering easy? 

[mlr3cluster](https://github.com/mlr-org/mlr3cluster) to the rescue!

mlr3cluster is a cluster analysis extention package within mlr3 library. It is 
a successsor of mlr's cluster capabilities in spirit and functionality. 

In order to understand the following mini-tutorial you need to be already 
familiar with R6 and basics of mlr3. 
See the [mlr3book (chapters 1-2)](https://mlr3book.mlr-org.com/) if you need a refresher. 

## Installation
To install the package, run the following code chunk:

```{r echo = TRUE, results = FALSE, message = FALSE, warning = FALSE}
devtools::install_github("https://github.com/mlr-org/mlr3cluster")
```

## Getting Started

Assuming you know all the basics and you've installed the package, 
here's an example on how to perform k-means clustering on a classic [usarrests](https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/USArrests.html) data set:

```{r intro}
library(mlr3)
library(mlr3cluster)

task = mlr_tasks$get("usarrests")
learner = mlr_learners$get("clust.kmeans")
learner$train(task)
preds = learner$predict(task = task)

preds
```

## Integrated Learners

List of integrated learners:

```{r}
mlr_learners$keys("clust")
```

The package contains all the basic types of clustering: partitional, hierarchial, density-based and fuzzy. 
Below is a detailed list of all the integrated learners.

| ID | Learner | Package |
| :--| :------ | :------ |
| [clust.agnes](https://mlr3cluster.mlr-org.com/reference/LearnerClustAgnes.html) | Agglomerative Hierarchical Clustering |  [cluster](https://CRAN.R-project.org/package=cluster) |
| [clust.cmeans](https://mlr3cluster.mlr-org.com/reference/LearnerClustCMeans.html) | Fuzzy C-Means Clustering |  [e1071](https://CRAN.R-project.org/package=e1071) |
| [clust.dbscan](https://mlr3cluster.mlr-org.com/reference/LearnerClustDBSCAN.html) | Density-based Clustering | [dbscan](https://CRAN.R-project.org/package=dbscan) |
| [clust.diana](https://mlr3cluster.mlr-org.com/reference/LearnerClustDiana.html) | Divisive Hierarchical Clustering | [cluster](https://CRAN.R-project.org/package=cluster) |
| [clust.fanny](https://mlr3cluster.mlr-org.com/reference/LearnerClustFanny.html) | Fuzzy Clustering | [cluster](https://CRAN.R-project.org/package=cluster) |
| [clust.featureless](https://mlr3cluster.mlr-org.com/reference/LearnerClustFeatureless.html) | Simple Featureless Clustering | [mlr3cluster](https://github.com/mlr-org/mlr3cluster) |
| [clust.kmeans](https://mlr3cluster.mlr-org.com/reference/LearnerClustKMeans.html) | K-Means Clustering | [stats](https://CRAN.R-project.org/package=stats) | 
| [clust.pam](https://mlr3cluster.mlr-org.com/reference/LearnerClustPAM.html) | Clustering Around Medoids | [cluster](https://CRAN.R-project.org/package=cluster) |
| [clust.xmeans](https://mlr3cluster.mlr-org.com/reference/LearnerClustXMeans.html) | K-Means with Automatic Determination of k | [RWeka](https://CRAN.R-project.org/package=RWeka) |


## Integrated Measures

List of integrated cluster measures: 

```{r}
mlr_measures$keys("clust")
```

Below is a detailed list of all the integrated learners.
look at clust.hb again to make sure name is correct. needs to be ch?

| ID | Measure | Package |
| :--| :------ | :------ |
| [clust.db](https://mlr3cluster.mlr-org.com/reference/MeasureClustInternal.html) | Davies-Bouldin Cluster Separation | [clusterCrit](https://CRAN.R-project.org/package=clusterCrit) |
| [clust.dunn](https://mlr3cluster.mlr-org.com/reference/MeasureClustInternal.html) | Dunn index | [clusterCrit](https://CRAN.R-project.org/package=clusterCrit) |
| [clust.ch](https://mlr3proba.mlr-org.com/reference/MeasureSurvCalibrationBeta.html) | Calinski Harabasz Pseudo F-Statistic | [clusterCrit](https://CRAN.R-project.org/package=clusterCrit) |
  | [clust.silhouette](https://mlr3cluster.mlr-org.com/reference/MeasureClustInternal.html) | Rousseeuw's Silhouette Quality Index | [clusterCrit](https://CRAN.R-project.org/package=clusterCrit) |


## Integrated Tasks

There is only one built-in Task in the package:

```{r}
mlr_tasks$get("usarrests")
```

As you can see, the biggest difference in clustering tasks and the rest of the tasks in mlr3 is the absense of the Target column. 


## Hyperparameters

Setting hyperparameters for clusterers is as easy as setting parameters for any other mlr3 learner: 

```{r hyperparam}
task = mlr_tasks$get("usarrests")
learner = mlr_learners$get("clust.kmeans")
learner$param_set
learner$param_set$values = list(centers = 3L, algorithm = "Lloyd", iter.max = 100L)
```


## Train and Predict
The "train" method is simply creating a model with cluster assignments for data, while 
the "predict" method's functionality varies depending on the clusterer in use. 
Read the documentation for more information. 

In the case of `kmeans` clusterer, the predict method is using `clue::cl_predict` which performs cluster assignments for new data. 

Following the example from the previous section:

```{r preds}
train_set = sample(task$nrow, 0.8 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
learner$train(task, row_ids = train_set)
preds = learner$predict(task, row_ids = test_set)
```

## Evaluation

Below is an example how to use built-in measures to access the quality of clustering assignment 

```{r}
measure = mlr_measures$get("clust.silhouette")
preds$score(measure, task)
```

  
## Visualization

(might be revised)
How do you visualize the tasks and the results of clustering experiments? 
I have also added additional plots to `mlr3viz` which help you visualize the analises.

```{r echo = TRUE, results = FALSE, message = FALSE, warning = FALSE}
library(mlr3viz)
autoplot(task)

autoplot(preds, task)

autoplot(preds, task, type = "sil")
```


## Acknowledgements
I would like to thank the following people for their help and guidance that they 
have provided throughout my work on the package: 
Michel Lang, Lars Kotthoff, Martin Binder, Patrick Stratz, Bernd Bischl. 


<!--chapter:end:2020-08-14-introducing-mlr3cluster-cluster-analysis-package.Rmd-->

